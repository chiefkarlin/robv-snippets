{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioNemo Deployment on Vertex\n",
    "## References\n",
    "* https://docs.nvidia.com/bionemo-framework/latest/deep-dive-esm1-pytriton-inference.html\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conda Setup\n",
    "```\n",
    "export CONDA_ENV=pytorch-nightly-conda\n",
    "conda create -p $CONDA_ENV pytorch ipykernel fastapi jupyter pytorch-cuda=12.1 python=3.11 -c pytorch -c nvidia -y\n",
    "$CONDA_EXE run -p $CONDA_ENV python -m ipykernel install --user --name=$CONDA_ENV\n",
    "$CONDA_EXE run -p $CONDA_ENV python -m pip install docker\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an NGC API Key, add to Secret Manager\n",
    "* Create an account at org.ngc.nvidia.com, then create a new API Key\n",
    "* Add it to secrets manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo -n \"<YOUR NGC API KEY>\" | gcloud secrets create ngc-api-key --data-file=-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NGC_SECRET= ! gcloud secrets versions access latest --secret=ngc-api-key\n",
    "NGC_API_KEY=NGC_SECRET[0]\n",
    "NGC_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NGC API Key, download the `esm1nv` model for generating embeddings from the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "response = requests.get('https://authn.nvidia.com/token?service=ngc&scope=group/ngc:', auth=('$oauthtoken', NGC_API_KEY))\n",
    "TOKEN = response.json()['token']\n",
    "\n",
    "MODEL = 'esm1nv'\n",
    "MODEL_FILE = f'{MODEL}.nemo'\n",
    "\n",
    "MODEL_URI = f'https://api.ngc.nvidia.com/v2/org/nvidia/team/clara/models/esm1nv/versions/1.0/files/{MODEL_FILE}'\n",
    "\n",
    "model_response= requests.get(MODEL_URI, stream=True,allow_redirects=True, headers={\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\"}\n",
    ")\n",
    "\n",
    "print(model_response)\n",
    "\n",
    "# Sizes in bytes.\n",
    "total_size = int(model_response.headers.get(\"content-length\", 0))\n",
    "block_size = 1024\n",
    "\n",
    "with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n",
    "    with open(MODEL_FILE, \"wb\") as file:\n",
    "        for data in model_response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "\n",
    "if total_size != 0 and progress_bar.n != total_size:\n",
    "    raise RuntimeError(\"Could not download model\")\n",
    "else:\n",
    "    print(f\"{MODEL_FILE} downloaded successfully!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize AI Platform SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex SDK\n",
    "PROJECT_ID='northam-ce-mlai-tpu'\n",
    "REGION='us-central1'\n",
    "BUCKET='robv-scratch'\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "aip.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering the NGC Repository as a `remote repository` in Artifact Regsitry\n",
    "First, we authorize our server account to have access to our repository secret for accessing NGC.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud projects add-iam-policy-binding northam-ce-mlai-tpu --member='serviceAccount:service-9452062936@gcp-sa-artifactregistry.iam.gserviceaccount.com' --role='roles/secretmanager.secretAccessor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try with NVidia NGC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud artifacts repositories create robv-test-ngc-remote-repo \\\n",
    "    --project=\"northam-ce-mlai-tpu\" \\\n",
    "    --repository-format=docker \\\n",
    "    --location=\"us-central1\" \\\n",
    "    --description=\"Nvidia GPU Containers\" \\\n",
    "    --mode=remote-repository \\\n",
    "    --remote-repo-config-desc=\"Nvidia remote NGC rpo \" \\\n",
    "    --remote-docker-repo=\"https://nvcr.io/\" \\\n",
    "    --remote-username=\"\\$oauthtoken\" \\\n",
    "    --remote-password-secret-version=\"projects/9452062936/secrets/ngc-api-key/versions/latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a standard repository for customized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud artifacts repositories create custom-container-prediction-vertex \\\n",
    "    --project=\"northam-ce-mlai-tpu\" \\\n",
    "    --repository-format=docker \\\n",
    "    --location=\"us-central1\" \\\n",
    "    --description=\"Custom serving containers for Vertex Prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud artifacts docker images list us-central1-docker.pkg.dev/northam-ce-mlai-tpu/robv-test-ngc-remote-repo/nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud artifacts repositories describe 'robv-test-ngc-remote-repo' --location=us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the container locally\n",
    "First, lets download the container from the repo and run it in our notebook using a Vertex LocalEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_PATH='us-central1-docker.pkg.dev/northam-ce-mlai-tpu/robv-test-ngc-remote-repo'\n",
    "IMAGE_URI=f'{REPO_PATH}/nvidia/clara/bionemo-framework:1.5'\n",
    "MODEL_ARTIFACTS_REPOSITORY=\"gs://robv-scratch/models/\"\n",
    "! docker pull $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch bionemo pytriton serving config\n",
    "In order to serve bionemo models using pytriton on Vertex AI, we have to patch the default `serve_bionemo_model.py` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serve_bionemo_model.py.patch\n",
    "--- serve_bionemo_model.py.orig 2024-07-03 19:45:15.549867973 +0000\n",
    "+++ serve_bionemo_model.py      2024-07-03 20:03:47.842013972 +0000\n",
    "@@ -16,7 +16,7 @@\n",
    " from model_navigator.package.package import Package\n",
    " from nemo.utils import logging\n",
    " from omegaconf import DictConfig\n",
    "-from pytriton.triton import Triton\n",
    "+from pytriton.triton import Triton, TritonConfig\n",
    " \n",
    " from bionemo.model.core.infer import M\n",
    " from bionemo.triton import decodes\n",
    "@@ -147,7 +147,10 @@\n",
    "     else:\n",
    "         maybe_model = None\n",
    " \n",
    "-    with Triton() as triton:\n",
    "+    ### robv@google.com - patch for Vertex\n",
    "+    config = TritonConfig(allow_http=True, allow_vertex_ai=True, vertex_ai_port=8080, vertex_ai_default_model=\"bionemo_model_embeddings\")\n",
    "+\n",
    "+    with Triton(config=config) as triton:\n",
    "         for maybe_triton_model_name, bind_fn in [\n",
    "             (embedding, bind_embedding),\n",
    "             (sampling, bind_sampling),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a new bionemo image with our patches/updates\n",
    "Pytriton support for vertex arrived in `nvidia-pytriton==0.5.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$IMAGE_URI\"\n",
    "echo $PWD\n",
    "echo $HOSTNAME\n",
    "echo $1\n",
    "\n",
    "# Change to scratch directory\n",
    "mkdir /mnt/localssd/scratch\n",
    "cp $PWD/esm1nv.nemo $PWD/serve_bionemo_model.py.patch /mnt/localssd/scratch && cd /mnt/localssd/scratch\n",
    "docker build --progress=plain --no-cache -t bionemo-esm1nv:1.5 -f - . <<EOF 2>&1 | tee build.log\n",
    "FROM $1\n",
    "COPY /esm1nv.nemo /workspace/bionemo/models/protein/esm1nv/\n",
    "COPY /serve_bionemo_model.py.patch /workspace/bionemo/bionemo/triton/\n",
    "RUN python -m pip install --upgrade nvidia-pytriton\n",
    "WORKDIR /workspace/bionemo/bionemo/triton\n",
    "RUN patch serve_bionemo_model.py serve_bionemo_model.py.patch \n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.prediction import LocalModel, LocalEndpoint\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "IMAGE_URI=\"bionemo-esm1nv:1.5\"\n",
    "\n",
    "\n",
    "local_model = LocalModel(\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_args=[\"python -m bionemo.triton.inference_wrapper --config-path /workspace/bionemo/examples/protein/esm1nv/conf\"],\n",
    "    serving_container_predict_route = \"/v2/models/bionemo_model_embeddings/infer\",\n",
    "    serving_container_health_route = \"/v2/models/bionemo_model_embeddings\",\n",
    ")\n",
    "\n",
    "local_endpoint = local_model.deploy_to_local_endpoint(\n",
    "    host_port=8080,\n",
    "    gpu_count=-1\n",
    ")\n",
    "\n",
    "local_endpoint.serve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sequences.json\n",
    "{\"inputs\":[\n",
    "        {\n",
    "            \"name\":\"sequences\",\n",
    "            \"data\": [\"MSLKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL\", \"MIQSQINRNIRLDLADAILLSKAKKDLSFAEIADGTGLA\"],\n",
    "            \"datatype\":\"BYTES\",\n",
    "            \"shape\":[2,1]\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('sequences.json') as f:\n",
    "    sequences = json.load(f)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests with `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST=\"localhost\"\n",
    "PORT=8080\n",
    "MODEL_PATH=\"bionemo_model_embeddings\"\n",
    "#MODEL_PATH=\"bionemo_model\"\n",
    "\n",
    "URL = f\"http://{HOST}:{PORT}/v2/models/{MODEL_PATH}/infer\"\n",
    "\n",
    "resp = requests.post(URL,data=json.dumps(sequences),headers={\n",
    "    \"Content-Type\": \"application/json; charset=UTF-8\"})\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with local endpoint predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = local_endpoint.predict(\n",
    "    request=json.dumps(sequences),\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy to new image to push to AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY='custom-container-prediction-vertex'\n",
    "IMAGE=IMAGE_URI\n",
    "\n",
    "local_model_ar = local_model.copy_image(\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_ar.get_serving_container_spec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_ar.push_image()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import Model\n",
    "\n",
    "model = Model.upload(\n",
    "    local_model=local_model_ar,\n",
    "    display_name=\"Bionemo esm1nv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(\n",
    "    machine_type=\"g2-standard-16\",\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    accelerator_count=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test default embeddings models in Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID=\"projects/9452062936/locations/us-central1/endpoints/5288859836811837440\"\n",
    "! gcloud ai endpoints raw-predict $ENDPOINT_ID \\\n",
    "  --region=us-central1 \\\n",
    "  --http-headers=Content-Type=application/json \\\n",
    "  --request @sequences.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test other model (hiddens) deployed on endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud ai endpoints raw-predict $ENDPOINT_ID \\\n",
    "  --region=us-central1 \\\n",
    "  --http-headers=Content-Type=application/json,\"X-Vertex-Ai-Triton-Redirect=v2/models/bionemo_model_hiddens/infer\" \\\n",
    "  --request @sequences.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-nightly-conda",
   "language": "python",
   "name": "pytorch-nightly-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
