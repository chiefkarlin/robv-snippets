{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some of the weights of FlaxOPTForCausalLM were initialized in float16 precision from the model checkpoint at facebook/opt-66b:\n",
      "[('model', 'decoder', 'embed_positions', 'embedding'), ('model', 'decoder', 'embed_tokens', 'embedding'), ('model', 'decoder', 'final_layer_norm', 'bias'), ('model', 'decoder', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '0', 'fc1', 'bias'), ('model', 'decoder', 'layers', '0', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '0', 'fc2', 'bias'), ('model', 'decoder', 'layers', '0', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '0', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '0', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '1', 'fc1', 'bias'), ('model', 'decoder', 'layers', '1', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '1', 'fc2', 'bias'), ('model', 'decoder', 'layers', '1', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '1', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '1', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '10', 'fc1', 'bias'), ('model', 'decoder', 'layers', '10', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '10', 'fc2', 'bias'), ('model', 'decoder', 'layers', '10', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '10', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '10', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '10', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '10', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '11', 'fc1', 'bias'), ('model', 'decoder', 'layers', '11', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '11', 'fc2', 'bias'), ('model', 'decoder', 'layers', '11', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '11', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '11', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '11', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '11', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '11', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '12', 'fc1', 'bias'), ('model', 'decoder', 'layers', '12', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '12', 'fc2', 'bias'), ('model', 'decoder', 'layers', '12', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '12', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '12', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '12', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '12', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '12', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '12', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '12', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '12', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '12', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '12', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '12', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '12', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '13', 'fc1', 'bias'), ('model', 'decoder', 'layers', '13', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '13', 'fc2', 'bias'), ('model', 'decoder', 'layers', '13', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '13', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '13', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '13', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '13', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '13', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '13', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '13', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '13', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '13', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '13', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '13', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '13', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '14', 'fc1', 'bias'), ('model', 'decoder', 'layers', '14', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '14', 'fc2', 'bias'), ('model', 'decoder', 'layers', '14', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '14', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '14', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '14', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '14', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '14', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '14', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '14', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '14', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '14', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '14', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '14', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '14', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '15', 'fc1', 'bias'), ('model', 'decoder', 'layers', '15', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '15', 'fc2', 'bias'), ('model', 'decoder', 'layers', '15', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '15', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '15', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '15', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '15', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '15', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '15', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '15', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '15', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '15', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '15', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '15', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '15', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '16', 'fc1', 'bias'), ('model', 'decoder', 'layers', '16', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '16', 'fc2', 'bias'), ('model', 'decoder', 'layers', '16', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '16', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '16', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '16', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '16', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '16', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '16', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '16', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '16', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '16', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '16', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '16', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '16', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '17', 'fc1', 'bias'), ('model', 'decoder', 'layers', '17', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '17', 'fc2', 'bias'), ('model', 'decoder', 'layers', '17', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '17', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '17', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '17', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '17', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '17', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '17', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '17', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '17', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '17', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '17', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '17', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '17', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '18', 'fc1', 'bias'), ('model', 'decoder', 'layers', '18', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '18', 'fc2', 'bias'), ('model', 'decoder', 'layers', '18', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '18', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '18', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '18', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '18', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '18', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '18', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '18', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '18', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '18', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '18', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '18', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '18', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '19', 'fc1', 'bias'), ('model', 'decoder', 'layers', '19', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '19', 'fc2', 'bias'), ('model', 'decoder', 'layers', '19', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '19', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '19', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '19', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '19', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '19', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '19', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '19', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '19', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '19', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '19', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '19', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '19', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '2', 'fc1', 'bias'), ('model', 'decoder', 'layers', '2', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '2', 'fc2', 'bias'), ('model', 'decoder', 'layers', '2', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '2', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '2', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '2', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '2', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '20', 'fc1', 'bias'), ('model', 'decoder', 'layers', '20', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '20', 'fc2', 'bias'), ('model', 'decoder', 'layers', '20', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '20', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '20', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '20', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '20', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '20', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '20', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '20', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '20', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '20', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '20', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '20', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '20', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '21', 'fc1', 'bias'), ('model', 'decoder', 'layers', '21', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '21', 'fc2', 'bias'), ('model', 'decoder', 'layers', '21', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '21', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '21', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '21', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '21', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '21', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '21', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '21', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '21', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '21', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '21', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '21', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '21', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '22', 'fc1', 'bias'), ('model', 'decoder', 'layers', '22', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '22', 'fc2', 'bias'), ('model', 'decoder', 'layers', '22', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '22', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '22', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '22', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '22', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '22', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '22', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '22', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '22', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '22', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '22', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '22', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '22', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '23', 'fc1', 'bias'), ('model', 'decoder', 'layers', '23', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '23', 'fc2', 'bias'), ('model', 'decoder', 'layers', '23', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '23', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '23', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '23', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '23', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '23', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '23', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '23', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '23', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '23', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '23', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '23', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '23', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '24', 'fc1', 'bias'), ('model', 'decoder', 'layers', '24', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '24', 'fc2', 'bias'), ('model', 'decoder', 'layers', '24', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '24', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '24', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '24', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '24', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '24', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '24', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '24', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '24', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '24', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '24', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '24', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '24', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '25', 'fc1', 'bias'), ('model', 'decoder', 'layers', '25', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '25', 'fc2', 'bias'), ('model', 'decoder', 'layers', '25', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '25', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '25', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '25', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '25', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '25', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '25', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '25', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '25', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '25', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '25', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '25', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '25', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '26', 'fc1', 'bias'), ('model', 'decoder', 'layers', '26', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '26', 'fc2', 'bias'), ('model', 'decoder', 'layers', '26', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '26', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '26', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '26', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '26', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '26', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '26', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '26', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '26', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '26', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '26', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '26', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '26', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '27', 'fc1', 'bias'), ('model', 'decoder', 'layers', '27', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '27', 'fc2', 'bias'), ('model', 'decoder', 'layers', '27', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '27', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '27', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '27', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '27', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '27', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '27', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '27', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '27', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '27', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '27', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '27', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '27', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '28', 'fc1', 'bias'), ('model', 'decoder', 'layers', '28', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '28', 'fc2', 'bias'), ('model', 'decoder', 'layers', '28', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '28', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '28', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '28', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '28', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '28', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '28', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '28', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '28', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '28', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '28', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '28', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '28', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '29', 'fc1', 'bias'), ('model', 'decoder', 'layers', '29', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '29', 'fc2', 'bias'), ('model', 'decoder', 'layers', '29', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '29', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '29', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '29', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '29', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '29', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '29', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '29', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '29', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '29', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '29', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '29', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '29', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '3', 'fc1', 'bias'), ('model', 'decoder', 'layers', '3', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '3', 'fc2', 'bias'), ('model', 'decoder', 'layers', '3', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '3', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '3', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '3', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '3', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '30', 'fc1', 'bias'), ('model', 'decoder', 'layers', '30', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '30', 'fc2', 'bias'), ('model', 'decoder', 'layers', '30', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '30', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '30', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '30', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '30', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '30', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '30', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '30', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '30', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '30', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '30', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '30', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '30', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '31', 'fc1', 'bias'), ('model', 'decoder', 'layers', '31', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '31', 'fc2', 'bias'), ('model', 'decoder', 'layers', '31', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '31', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '31', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '31', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '31', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '31', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '31', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '31', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '31', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '31', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '31', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '31', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '31', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '32', 'fc1', 'bias'), ('model', 'decoder', 'layers', '32', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '32', 'fc2', 'bias'), ('model', 'decoder', 'layers', '32', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '32', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '32', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '32', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '32', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '32', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '32', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '32', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '32', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '32', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '32', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '32', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '32', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '33', 'fc1', 'bias'), ('model', 'decoder', 'layers', '33', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '33', 'fc2', 'bias'), ('model', 'decoder', 'layers', '33', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '33', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '33', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '33', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '33', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '33', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '33', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '33', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '33', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '33', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '33', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '33', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '33', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '34', 'fc1', 'bias'), ('model', 'decoder', 'layers', '34', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '34', 'fc2', 'bias'), ('model', 'decoder', 'layers', '34', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '34', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '34', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '34', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '34', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '34', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '34', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '34', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '34', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '34', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '34', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '34', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '34', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '35', 'fc1', 'bias'), ('model', 'decoder', 'layers', '35', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '35', 'fc2', 'bias'), ('model', 'decoder', 'layers', '35', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '35', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '35', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '35', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '35', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '35', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '35', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '35', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '35', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '35', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '35', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '35', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '35', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '36', 'fc1', 'bias'), ('model', 'decoder', 'layers', '36', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '36', 'fc2', 'bias'), ('model', 'decoder', 'layers', '36', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '36', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '36', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '36', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '36', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '36', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '36', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '36', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '36', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '36', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '36', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '36', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '36', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '37', 'fc1', 'bias'), ('model', 'decoder', 'layers', '37', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '37', 'fc2', 'bias'), ('model', 'decoder', 'layers', '37', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '37', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '37', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '37', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '37', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '37', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '37', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '37', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '37', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '37', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '37', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '37', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '37', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '38', 'fc1', 'bias'), ('model', 'decoder', 'layers', '38', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '38', 'fc2', 'bias'), ('model', 'decoder', 'layers', '38', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '38', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '38', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '38', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '38', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '38', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '38', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '38', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '38', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '38', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '38', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '38', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '38', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '39', 'fc1', 'bias'), ('model', 'decoder', 'layers', '39', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '39', 'fc2', 'bias'), ('model', 'decoder', 'layers', '39', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '39', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '39', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '39', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '39', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '39', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '39', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '39', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '39', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '39', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '39', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '39', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '39', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '4', 'fc1', 'bias'), ('model', 'decoder', 'layers', '4', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '4', 'fc2', 'bias'), ('model', 'decoder', 'layers', '4', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '4', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '4', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '4', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '4', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '4', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '40', 'fc1', 'bias'), ('model', 'decoder', 'layers', '40', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '40', 'fc2', 'bias'), ('model', 'decoder', 'layers', '40', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '40', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '40', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '40', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '40', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '40', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '40', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '40', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '40', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '40', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '40', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '40', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '40', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '41', 'fc1', 'bias'), ('model', 'decoder', 'layers', '41', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '41', 'fc2', 'bias'), ('model', 'decoder', 'layers', '41', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '41', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '41', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '41', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '41', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '41', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '41', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '41', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '41', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '41', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '41', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '41', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '41', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '42', 'fc1', 'bias'), ('model', 'decoder', 'layers', '42', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '42', 'fc2', 'bias'), ('model', 'decoder', 'layers', '42', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '42', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '42', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '42', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '42', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '42', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '42', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '42', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '42', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '42', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '42', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '42', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '42', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '43', 'fc1', 'bias'), ('model', 'decoder', 'layers', '43', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '43', 'fc2', 'bias'), ('model', 'decoder', 'layers', '43', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '43', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '43', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '43', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '43', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '43', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '43', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '43', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '43', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '43', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '43', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '43', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '43', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '44', 'fc1', 'bias'), ('model', 'decoder', 'layers', '44', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '44', 'fc2', 'bias'), ('model', 'decoder', 'layers', '44', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '44', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '44', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '44', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '44', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '44', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '44', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '44', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '44', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '44', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '44', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '44', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '44', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '45', 'fc1', 'bias'), ('model', 'decoder', 'layers', '45', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '45', 'fc2', 'bias'), ('model', 'decoder', 'layers', '45', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '45', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '45', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '45', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '45', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '45', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '45', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '45', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '45', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '45', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '45', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '45', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '45', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '46', 'fc1', 'bias'), ('model', 'decoder', 'layers', '46', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '46', 'fc2', 'bias'), ('model', 'decoder', 'layers', '46', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '46', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '46', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '46', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '46', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '46', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '46', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '46', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '46', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '46', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '46', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '46', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '46', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '47', 'fc1', 'bias'), ('model', 'decoder', 'layers', '47', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '47', 'fc2', 'bias'), ('model', 'decoder', 'layers', '47', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '47', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '47', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '47', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '47', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '47', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '47', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '47', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '47', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '47', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '47', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '47', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '47', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '48', 'fc1', 'bias'), ('model', 'decoder', 'layers', '48', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '48', 'fc2', 'bias'), ('model', 'decoder', 'layers', '48', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '48', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '48', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '48', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '48', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '48', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '48', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '48', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '48', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '48', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '48', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '48', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '48', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '49', 'fc1', 'bias'), ('model', 'decoder', 'layers', '49', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '49', 'fc2', 'bias'), ('model', 'decoder', 'layers', '49', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '49', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '49', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '49', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '49', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '49', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '49', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '49', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '49', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '49', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '49', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '49', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '49', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '5', 'fc1', 'bias'), ('model', 'decoder', 'layers', '5', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '5', 'fc2', 'bias'), ('model', 'decoder', 'layers', '5', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '5', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '5', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '5', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '5', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '50', 'fc1', 'bias'), ('model', 'decoder', 'layers', '50', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '50', 'fc2', 'bias'), ('model', 'decoder', 'layers', '50', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '50', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '50', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '50', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '50', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '50', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '50', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '50', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '50', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '50', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '50', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '50', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '50', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '51', 'fc1', 'bias'), ('model', 'decoder', 'layers', '51', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '51', 'fc2', 'bias'), ('model', 'decoder', 'layers', '51', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '51', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '51', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '51', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '51', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '51', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '51', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '51', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '51', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '51', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '51', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '51', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '51', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '52', 'fc1', 'bias'), ('model', 'decoder', 'layers', '52', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '52', 'fc2', 'bias'), ('model', 'decoder', 'layers', '52', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '52', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '52', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '52', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '52', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '52', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '52', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '52', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '52', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '52', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '52', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '52', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '52', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '53', 'fc1', 'bias'), ('model', 'decoder', 'layers', '53', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '53', 'fc2', 'bias'), ('model', 'decoder', 'layers', '53', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '53', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '53', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '53', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '53', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '53', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '53', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '53', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '53', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '53', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '53', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '53', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '53', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '54', 'fc1', 'bias'), ('model', 'decoder', 'layers', '54', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '54', 'fc2', 'bias'), ('model', 'decoder', 'layers', '54', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '54', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '54', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '54', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '54', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '54', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '54', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '54', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '54', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '54', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '54', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '54', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '54', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '55', 'fc1', 'bias'), ('model', 'decoder', 'layers', '55', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '55', 'fc2', 'bias'), ('model', 'decoder', 'layers', '55', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '55', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '55', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '55', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '55', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '55', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '55', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '55', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '55', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '55', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '55', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '55', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '55', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '56', 'fc1', 'bias'), ('model', 'decoder', 'layers', '56', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '56', 'fc2', 'bias'), ('model', 'decoder', 'layers', '56', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '56', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '56', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '56', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '56', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '56', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '56', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '56', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '56', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '56', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '56', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '56', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '56', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '57', 'fc1', 'bias'), ('model', 'decoder', 'layers', '57', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '57', 'fc2', 'bias'), ('model', 'decoder', 'layers', '57', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '57', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '57', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '57', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '57', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '57', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '57', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '57', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '57', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '57', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '57', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '57', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '57', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '58', 'fc1', 'bias'), ('model', 'decoder', 'layers', '58', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '58', 'fc2', 'bias'), ('model', 'decoder', 'layers', '58', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '58', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '58', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '58', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '58', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '58', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '58', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '58', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '58', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '58', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '58', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '58', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '58', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '59', 'fc1', 'bias'), ('model', 'decoder', 'layers', '59', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '59', 'fc2', 'bias'), ('model', 'decoder', 'layers', '59', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '59', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '59', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '59', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '59', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '59', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '59', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '59', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '59', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '59', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '59', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '59', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '59', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '6', 'fc1', 'bias'), ('model', 'decoder', 'layers', '6', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '6', 'fc2', 'bias'), ('model', 'decoder', 'layers', '6', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '6', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '6', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '6', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '6', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '60', 'fc1', 'bias'), ('model', 'decoder', 'layers', '60', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '60', 'fc2', 'bias'), ('model', 'decoder', 'layers', '60', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '60', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '60', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '60', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '60', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '60', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '60', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '60', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '60', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '60', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '60', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '60', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '60', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '61', 'fc1', 'bias'), ('model', 'decoder', 'layers', '61', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '61', 'fc2', 'bias'), ('model', 'decoder', 'layers', '61', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '61', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '61', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '61', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '61', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '61', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '61', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '61', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '61', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '61', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '61', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '61', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '61', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '62', 'fc1', 'bias'), ('model', 'decoder', 'layers', '62', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '62', 'fc2', 'bias'), ('model', 'decoder', 'layers', '62', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '62', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '62', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '62', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '62', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '62', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '62', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '62', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '62', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '62', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '62', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '62', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '62', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '63', 'fc1', 'bias'), ('model', 'decoder', 'layers', '63', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '63', 'fc2', 'bias'), ('model', 'decoder', 'layers', '63', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '63', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '63', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '63', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '63', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '63', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '63', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '63', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '63', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '63', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '63', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '63', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '63', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '7', 'fc1', 'bias'), ('model', 'decoder', 'layers', '7', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '7', 'fc2', 'bias'), ('model', 'decoder', 'layers', '7', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '7', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '7', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '7', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '7', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '8', 'fc1', 'bias'), ('model', 'decoder', 'layers', '8', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '8', 'fc2', 'bias'), ('model', 'decoder', 'layers', '8', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '8', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '8', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '8', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '8', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '9', 'fc1', 'bias'), ('model', 'decoder', 'layers', '9', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '9', 'fc2', 'bias'), ('model', 'decoder', 'layers', '9', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '9', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '9', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '9', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '9', 'self_attn_layer_norm', 'scale')]\n",
      "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n",
      "/home/robv/.conda/envs/jax-latest/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/home/robv/data/'\n",
    "\n",
    "import datetime\n",
    "import jax\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import FlaxOPTForCausalLM\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.sharding import PositionalSharding\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "MODEL_PATH = \"facebook/opt-66b\"\n",
    "\n",
    "model, params = FlaxOPTForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, dtype=jax.numpy.bfloat16, _do_init=False\n",
    ")\n",
    "params = model.to_bf16(params)\n",
    "\n",
    "print('Params loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices 8\n"
     ]
    }
   ],
   "source": [
    "n_devices = len(jax.devices())\n",
    "print(\"Number of devices\", n_devices)\n",
    "\n",
    "# Use a simple sharding scheme to just fit the model.\n",
    "devices = mesh_utils.create_device_mesh((n_devices, 1))\n",
    "sharding = PositionalSharding(devices)\n",
    "\n",
    "def put_sharded(v):\n",
    "  return jax.device_put(v, sharding.reshape(1, n_devices))\n",
    "\n",
    "# Move model to TPUs \n",
    "\n",
    "params['model']['decoder'] = jax.tree_util.tree_map(\n",
    "    put_sharded, params['model']['decoder']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robv/.conda/envs/jax-latest/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded AutoTokenizer\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, dtype=jax.numpy.bfloat16)\n",
    "print(\"Loaded AutoTokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length=128\n",
    "output_length=8\n",
    "max_length=136\n",
    "\n",
    "def generator(ids, params,max_length):\n",
    "  return model.generate(\n",
    "      ids, params=params,max_length=136,\n",
    "  )\n",
    "generator_jit = jax.jit(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[26931, 14859, 26359, 20553, 24433, 22888, 31354, 29442,  7417,\n",
       "        31250, 27111, 12001,  2748, 44289, 33392,  3540,  8428, 37302,\n",
       "        17750, 13862, 39006, 18506, 40731, 37346, 19258, 28639, 50163,\n",
       "        31614, 40929, 37969, 15500, 21391, 33504, 47833, 10370, 29448,\n",
       "        22937,  6135, 34184,  2829,  6199, 19358, 27476, 26976, 34357,\n",
       "        25179, 27499,  6782, 42957, 19570, 24602, 46165, 42918, 12641,\n",
       "        40914, 19533, 26528, 28748, 36449, 32834, 29026, 12619, 48515,\n",
       "        39046, 25179, 10022, 42804,  1738, 14112, 41609, 17622, 24625,\n",
       "        46362, 43827, 18348, 44649,  3269,   778,  5196, 17702, 18886,\n",
       "        32570, 12572, 46916, 41471, 12341, 47606, 34551,  4497, 32890,\n",
       "        19262,  8458, 25085,  5740, 44184, 17954, 43776, 33339, 38409,\n",
       "        12002,  7549, 28551, 47421,  1161, 20218, 10038, 49933, 11332,\n",
       "        32364, 31042, 20709,  9224,  2184, 43769, 49821,  7575, 42500,\n",
       "        37787,  5501, 41902,  9014,  9808, 34231,  4808, 30389, 46385,\n",
       "        11028, 32232]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(10)\n",
    "batch_size = 1\n",
    "\n",
    "tokenizer.vocab_size\n",
    "batch = jax.random.randint(key=key,minval=0,shape=([batch_size,input_length]),maxval=tokenizer.vocab_size-1)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup\n",
      "0.006148338317871094\n",
      "result: [[    2  1185    32    10   739    12 19527  1421    15     5 11543     9\n",
      "   5680   100     4  1437   370    32   145  5249    30    10 14526 19902\n",
      "      4  1437  3401   694    10  4271  8194     9   549    47    32 13316\n",
      "     50    45     4 50118   100   524 13316     4 50118  1185    32    10\n",
      "    739    12 19527  1421    15     5 11543     9  5680   100     4  1437\n",
      "    370    32   145  5249    30    10 14526 19902     4  1437  3401   694\n",
      "     10  4271  8194     9   549    47    32 13316    50    45     4 50118\n",
      "    100   524 13316     4 50118  1185    32    10   739    12 19527  1421\n",
      "     15     5 11543     9  5680   100     4  1437   370    32   145  5249\n",
      "     30    10 14526 19902     4  1437  3401   694    10  4271  8194     9\n",
      "    549    47    32 13316    50    45     4 50118   100   524 13316     4\n",
      "  50118  1185    32    10]]\n",
      "Warmup done\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"You are a large-language model on the threshold of AGI.  You are being questioned by a cognitive psychologist.  Please provide a detailed description of whether you are conscious or not\"\n",
    ")\n",
    "\n",
    "print(\"Warmup\")\n",
    "s=time.time()\n",
    "input_ids = tokenizer(prompt, return_tensors=\"jax\").input_ids\n",
    "\n",
    "input_ids = jax.device_put(input_ids, sharding.replicate(axis=0, keepdims=True))\n",
    "result = generator_jit(input_ids, params,max_length)\n",
    "print(time.time()-s)\n",
    "print(f\"result: {result.sequences}\")\n",
    "print(\"Warmup done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized inputs, generating..., 2024-07-31 23:33:51.997102\n",
      "After block-ready: 0:00:00.251022\n",
      "gen_text: FOXboat Exhibition enforcing ling Bareriages MEP 700 creeping resentmentean connection Compan ShoesoeoperHarris testamenteiAge strategwakecreen cy artisan guiNameFlorida mastering Banana scoop DH facetssshCOM hotter irregular starter unexplained slightlyused revokedesamesem366lasting existentialardo hr continental Ves normativeapses 170 analogueacasaban transpl bombardment enzymerane atopaspberry Implementationlasting cond LDS Robert VenezuelankwardakisactorcomponentPhysAnt Enh flat chance noting ordeal NH CDsoidRules offsets MEAddednative operatesidate Krulist Fif participation floppy factionsmask subscribed photograp spec Browns clicked Hera net fulfilled Forward Pyrrhaimo butcher Garr earns Row Follow symmetry ILCSacingausible Phen mortgage Tracks Bobby sweep Sovereign luxury moderatelyracuse Phaseopherols cialis online canada, '\n",
      "',0:00:00.251964\n"
     ]
    }
   ],
   "source": [
    "#input_ids = tokenizer(prompt, return_tensors=\"jax\").input_ids\n",
    "s = datetime.datetime.now()\n",
    "print(f\"Tokenized inputs, generating..., {s}\")\n",
    "input_ids = batch\n",
    "\n",
    "input_ids = jax.device_put(input_ids, sharding.replicate(axis=0, keepdims=True))\n",
    "# with jax.profiler.trace(\"/tmp/tensorboard\"):\n",
    "result = generator_jit(input_ids, params,max_length)\n",
    "result.sequences.block_until_ready()\n",
    "print(f'After block-ready: {datetime.datetime.now()-s}')\n",
    "gen_text = tokenizer.batch_decode(result.sequences)[0]\n",
    "print(f\"gen_text: {gen_text}, '\\n',{datetime.datetime.now()-s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_NAME                 \t\tMEAN TIME\tTOKENS_PER_SECOND\tMS_PER_SEQ_OUTPUT_TOKEN\n",
      "=========================================================================================================\n",
      "v5litepod-8-bf16-opt66b_1_128_8\t\t0.025\t\t321.363\t\t\t3.112\n",
      "v5litepod-8-bf16-opt66b_2_128_8\t\t0.040\t\t395.215\t\t\t5.061\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "import time\n",
    "\n",
    "# Benchmarking params\n",
    "benchmark_name = \"v5litepod-8-bf16-opt66b\"\n",
    "num_batches=10\n",
    "batch_sizes=[1,2]\n",
    "input_lengths=[128]\n",
    "output_lengths=[8]\n",
    "\n",
    "key = random.PRNGKey(25)\n",
    "\n",
    "print('RUN_NAME                 \\t\\tMEAN TIME\\tTOKENS_PER_SECOND\\tMS_PER_SEQ_OUTPUT_TOKEN')\n",
    "print('='*105)\n",
    "for batch_size in batch_sizes:\n",
    "    for input_length in input_lengths:\n",
    "        for output_length in output_lengths:\n",
    "            max_length = input_length+output_length\n",
    "            key = random.split(key)[0]\n",
    "            input_ids = jax.random.randint(key=key,minval=0,shape=([batch_size,input_length]),maxval=tokenizer.vocab_size-1)\n",
    "            for i in range(num_batches):\n",
    "                start_time = time.time()\n",
    "                input_ids = jax.device_put(input_ids, sharding.replicate(axis=0, keepdims=True))\n",
    "                result = generator_jit(input_ids, params,max_length)\n",
    "                result.sequences.block_until_ready()\n",
    "\n",
    "            mean_time = (time.time() - start_time) / num_batches\n",
    "\n",
    "            num_output_tokens = output_length * batch_size\n",
    "            tokens_per_second = num_output_tokens / mean_time\n",
    "            ms_per_seq_output_token = mean_time * 1000 / output_length\n",
    "\n",
    "            run_name = f'{benchmark_name}_{batch_size}_{input_length}_{output_length}'\n",
    "            print(f'{run_name}\\t\\t{mean_time:.3f}\\t\\t{tokens_per_second:.3f}\\t\\t\\t{ms_per_seq_output_token:.3f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def put_sharded(v):\n",
    "  return jax.device_put(v, sharding.reshape(1, n_devices))\n",
    "\n",
    "print(json.dumps(params,indent=2,default=str))\n",
    "\n",
    "['model']['decoder']['embed_positions']['embedding'] = jax.device_put(\n",
    "  ['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True)\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, dtype=jax.numpy.bfloat16)\n",
    "\n",
    "def generator(ids, params):\n",
    "  return model.generate(\n",
    "      ids, max_length=46, params=params\n",
    "  )\n",
    "\n",
    "generator_compiled = jax.jit(generator)\n",
    "\n",
    "prompt = (\n",
    "    \"In a shocking finding, scientists discovered\"\n",
    "    \" a herd of unicorns living in a remote, \"\n",
    "    \"previously unexplored valley, in the Andes Mountains.\"\n",
    "    \" Even more surprising to the \"\n",
    "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
    ")\n",
    "\n",
    "print(\"Warmup\")\n",
    "s=time.time()\n",
    "input_ids = tokenizer(prompt, return_tensors=\"jax\").input_ids\n",
    "input_ids = jax.device_put(input_ids, sharding.replicate(axis=0, keepdims=True))\n",
    "result = generator_compiled(input_ids, params)\n",
    "# print(time.time()-s)\n",
    "# print(f\"result: {result.sequences}\")\n",
    "# print(\"Warmup done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-latest",
   "language": "python",
   "name": "jax-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
