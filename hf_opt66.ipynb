{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/robv/.conda/envs/jax-latest/lib/python3.10/site-packages (from transformers) (1.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/robv/.conda/envs/jax-latest/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, safetensors, urllib3, typing-extensions, tqdm, regex, pyyaml, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, transformers\n",
      "Successfully installed certifi-2023.5.7 charset-normalizer-3.2.0 filelock-3.12.2 fsspec-2023.6.0 huggingface-hub-0.16.4 idna-3.4 pyyaml-6.0 regex-2023.6.3 requests-2.31.0 safetensors-0.3.1 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.30.2 typing-extensions-4.7.1 urllib3-2.0.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some of the weights of FlaxOPTForCausalLM were initialized in float16 precision from the model checkpoint at facebook/opt-66b:\n",
      "[('model', 'decoder', 'embed_positions', 'embedding'), ('model', 'decoder', 'embed_tokens', 'embedding'), ('model', 'decoder', 'final_layer_norm', 'bias'), ('model', 'decoder', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '0', 'fc1', 'bias'), ('model', 'decoder', 'layers', '0', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '0', 'fc2', 'bias'), ('model', 'decoder', 'layers', '0', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '0', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '0', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '0', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '1', 'fc1', 'bias'), ('model', 'decoder', 'layers', '1', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '1', 'fc2', 'bias'), ('model', 'decoder', 'layers', '1', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '1', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '1', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '1', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '10', 'fc1', 'bias'), ('model', 'decoder', 'layers', '10', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '10', 'fc2', 'bias'), ('model', 'decoder', 'layers', '10', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '10', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '10', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '10', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '10', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '11', 'fc1', 'bias'), ('model', 'decoder', 'layers', '11', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '11', 'fc2', 'bias'), ('model', 'decoder', 'layers', '11', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '11', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '11', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '11', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '11', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '11', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '12', 'fc1', 'bias'), ('model', 'decoder', 'layers', '12', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '12', 'fc2', 'bias'), ('model', 'decoder', 'layers', '12', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '12', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '12', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '12', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '12', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '12', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '12', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '12', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '12', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '12', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '12', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '12', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '12', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '13', 'fc1', 'bias'), ('model', 'decoder', 'layers', '13', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '13', 'fc2', 'bias'), ('model', 'decoder', 'layers', '13', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '13', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '13', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '13', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '13', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '13', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '13', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '13', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '13', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '13', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '13', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '13', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '13', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '14', 'fc1', 'bias'), ('model', 'decoder', 'layers', '14', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '14', 'fc2', 'bias'), ('model', 'decoder', 'layers', '14', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '14', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '14', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '14', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '14', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '14', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '14', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '14', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '14', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '14', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '14', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '14', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '14', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '15', 'fc1', 'bias'), ('model', 'decoder', 'layers', '15', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '15', 'fc2', 'bias'), ('model', 'decoder', 'layers', '15', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '15', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '15', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '15', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '15', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '15', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '15', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '15', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '15', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '15', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '15', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '15', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '15', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '16', 'fc1', 'bias'), ('model', 'decoder', 'layers', '16', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '16', 'fc2', 'bias'), ('model', 'decoder', 'layers', '16', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '16', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '16', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '16', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '16', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '16', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '16', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '16', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '16', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '16', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '16', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '16', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '16', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '17', 'fc1', 'bias'), ('model', 'decoder', 'layers', '17', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '17', 'fc2', 'bias'), ('model', 'decoder', 'layers', '17', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '17', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '17', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '17', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '17', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '17', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '17', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '17', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '17', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '17', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '17', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '17', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '17', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '18', 'fc1', 'bias'), ('model', 'decoder', 'layers', '18', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '18', 'fc2', 'bias'), ('model', 'decoder', 'layers', '18', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '18', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '18', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '18', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '18', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '18', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '18', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '18', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '18', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '18', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '18', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '18', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '18', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '19', 'fc1', 'bias'), ('model', 'decoder', 'layers', '19', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '19', 'fc2', 'bias'), ('model', 'decoder', 'layers', '19', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '19', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '19', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '19', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '19', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '19', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '19', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '19', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '19', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '19', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '19', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '19', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '19', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '2', 'fc1', 'bias'), ('model', 'decoder', 'layers', '2', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '2', 'fc2', 'bias'), ('model', 'decoder', 'layers', '2', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '2', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '2', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '2', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '2', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '20', 'fc1', 'bias'), ('model', 'decoder', 'layers', '20', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '20', 'fc2', 'bias'), ('model', 'decoder', 'layers', '20', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '20', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '20', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '20', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '20', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '20', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '20', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '20', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '20', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '20', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '20', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '20', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '20', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '21', 'fc1', 'bias'), ('model', 'decoder', 'layers', '21', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '21', 'fc2', 'bias'), ('model', 'decoder', 'layers', '21', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '21', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '21', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '21', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '21', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '21', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '21', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '21', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '21', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '21', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '21', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '21', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '21', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '22', 'fc1', 'bias'), ('model', 'decoder', 'layers', '22', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '22', 'fc2', 'bias'), ('model', 'decoder', 'layers', '22', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '22', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '22', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '22', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '22', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '22', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '22', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '22', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '22', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '22', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '22', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '22', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '22', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '23', 'fc1', 'bias'), ('model', 'decoder', 'layers', '23', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '23', 'fc2', 'bias'), ('model', 'decoder', 'layers', '23', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '23', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '23', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '23', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '23', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '23', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '23', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '23', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '23', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '23', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '23', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '23', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '23', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '24', 'fc1', 'bias'), ('model', 'decoder', 'layers', '24', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '24', 'fc2', 'bias'), ('model', 'decoder', 'layers', '24', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '24', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '24', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '24', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '24', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '24', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '24', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '24', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '24', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '24', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '24', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '24', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '24', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '25', 'fc1', 'bias'), ('model', 'decoder', 'layers', '25', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '25', 'fc2', 'bias'), ('model', 'decoder', 'layers', '25', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '25', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '25', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '25', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '25', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '25', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '25', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '25', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '25', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '25', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '25', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '25', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '25', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '26', 'fc1', 'bias'), ('model', 'decoder', 'layers', '26', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '26', 'fc2', 'bias'), ('model', 'decoder', 'layers', '26', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '26', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '26', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '26', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '26', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '26', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '26', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '26', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '26', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '26', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '26', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '26', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '26', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '27', 'fc1', 'bias'), ('model', 'decoder', 'layers', '27', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '27', 'fc2', 'bias'), ('model', 'decoder', 'layers', '27', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '27', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '27', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '27', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '27', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '27', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '27', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '27', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '27', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '27', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '27', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '27', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '27', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '28', 'fc1', 'bias'), ('model', 'decoder', 'layers', '28', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '28', 'fc2', 'bias'), ('model', 'decoder', 'layers', '28', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '28', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '28', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '28', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '28', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '28', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '28', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '28', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '28', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '28', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '28', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '28', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '28', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '29', 'fc1', 'bias'), ('model', 'decoder', 'layers', '29', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '29', 'fc2', 'bias'), ('model', 'decoder', 'layers', '29', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '29', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '29', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '29', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '29', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '29', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '29', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '29', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '29', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '29', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '29', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '29', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '29', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '3', 'fc1', 'bias'), ('model', 'decoder', 'layers', '3', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '3', 'fc2', 'bias'), ('model', 'decoder', 'layers', '3', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '3', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '3', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '3', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '3', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '30', 'fc1', 'bias'), ('model', 'decoder', 'layers', '30', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '30', 'fc2', 'bias'), ('model', 'decoder', 'layers', '30', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '30', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '30', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '30', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '30', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '30', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '30', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '30', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '30', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '30', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '30', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '30', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '30', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '31', 'fc1', 'bias'), ('model', 'decoder', 'layers', '31', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '31', 'fc2', 'bias'), ('model', 'decoder', 'layers', '31', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '31', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '31', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '31', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '31', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '31', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '31', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '31', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '31', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '31', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '31', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '31', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '31', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '32', 'fc1', 'bias'), ('model', 'decoder', 'layers', '32', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '32', 'fc2', 'bias'), ('model', 'decoder', 'layers', '32', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '32', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '32', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '32', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '32', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '32', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '32', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '32', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '32', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '32', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '32', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '32', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '32', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '33', 'fc1', 'bias'), ('model', 'decoder', 'layers', '33', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '33', 'fc2', 'bias'), ('model', 'decoder', 'layers', '33', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '33', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '33', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '33', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '33', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '33', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '33', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '33', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '33', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '33', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '33', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '33', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '33', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '34', 'fc1', 'bias'), ('model', 'decoder', 'layers', '34', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '34', 'fc2', 'bias'), ('model', 'decoder', 'layers', '34', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '34', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '34', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '34', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '34', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '34', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '34', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '34', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '34', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '34', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '34', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '34', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '34', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '35', 'fc1', 'bias'), ('model', 'decoder', 'layers', '35', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '35', 'fc2', 'bias'), ('model', 'decoder', 'layers', '35', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '35', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '35', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '35', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '35', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '35', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '35', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '35', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '35', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '35', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '35', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '35', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '35', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '36', 'fc1', 'bias'), ('model', 'decoder', 'layers', '36', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '36', 'fc2', 'bias'), ('model', 'decoder', 'layers', '36', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '36', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '36', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '36', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '36', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '36', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '36', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '36', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '36', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '36', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '36', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '36', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '36', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '37', 'fc1', 'bias'), ('model', 'decoder', 'layers', '37', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '37', 'fc2', 'bias'), ('model', 'decoder', 'layers', '37', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '37', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '37', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '37', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '37', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '37', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '37', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '37', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '37', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '37', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '37', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '37', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '37', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '38', 'fc1', 'bias'), ('model', 'decoder', 'layers', '38', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '38', 'fc2', 'bias'), ('model', 'decoder', 'layers', '38', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '38', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '38', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '38', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '38', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '38', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '38', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '38', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '38', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '38', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '38', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '38', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '38', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '39', 'fc1', 'bias'), ('model', 'decoder', 'layers', '39', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '39', 'fc2', 'bias'), ('model', 'decoder', 'layers', '39', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '39', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '39', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '39', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '39', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '39', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '39', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '39', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '39', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '39', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '39', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '39', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '39', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '4', 'fc1', 'bias'), ('model', 'decoder', 'layers', '4', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '4', 'fc2', 'bias'), ('model', 'decoder', 'layers', '4', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '4', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '4', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '4', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '4', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '4', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '40', 'fc1', 'bias'), ('model', 'decoder', 'layers', '40', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '40', 'fc2', 'bias'), ('model', 'decoder', 'layers', '40', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '40', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '40', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '40', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '40', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '40', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '40', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '40', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '40', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '40', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '40', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '40', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '40', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '41', 'fc1', 'bias'), ('model', 'decoder', 'layers', '41', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '41', 'fc2', 'bias'), ('model', 'decoder', 'layers', '41', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '41', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '41', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '41', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '41', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '41', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '41', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '41', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '41', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '41', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '41', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '41', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '41', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '42', 'fc1', 'bias'), ('model', 'decoder', 'layers', '42', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '42', 'fc2', 'bias'), ('model', 'decoder', 'layers', '42', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '42', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '42', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '42', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '42', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '42', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '42', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '42', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '42', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '42', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '42', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '42', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '42', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '43', 'fc1', 'bias'), ('model', 'decoder', 'layers', '43', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '43', 'fc2', 'bias'), ('model', 'decoder', 'layers', '43', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '43', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '43', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '43', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '43', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '43', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '43', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '43', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '43', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '43', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '43', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '43', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '43', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '44', 'fc1', 'bias'), ('model', 'decoder', 'layers', '44', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '44', 'fc2', 'bias'), ('model', 'decoder', 'layers', '44', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '44', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '44', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '44', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '44', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '44', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '44', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '44', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '44', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '44', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '44', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '44', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '44', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '45', 'fc1', 'bias'), ('model', 'decoder', 'layers', '45', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '45', 'fc2', 'bias'), ('model', 'decoder', 'layers', '45', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '45', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '45', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '45', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '45', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '45', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '45', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '45', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '45', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '45', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '45', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '45', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '45', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '46', 'fc1', 'bias'), ('model', 'decoder', 'layers', '46', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '46', 'fc2', 'bias'), ('model', 'decoder', 'layers', '46', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '46', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '46', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '46', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '46', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '46', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '46', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '46', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '46', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '46', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '46', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '46', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '46', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '47', 'fc1', 'bias'), ('model', 'decoder', 'layers', '47', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '47', 'fc2', 'bias'), ('model', 'decoder', 'layers', '47', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '47', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '47', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '47', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '47', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '47', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '47', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '47', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '47', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '47', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '47', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '47', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '47', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '48', 'fc1', 'bias'), ('model', 'decoder', 'layers', '48', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '48', 'fc2', 'bias'), ('model', 'decoder', 'layers', '48', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '48', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '48', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '48', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '48', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '48', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '48', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '48', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '48', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '48', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '48', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '48', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '48', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '49', 'fc1', 'bias'), ('model', 'decoder', 'layers', '49', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '49', 'fc2', 'bias'), ('model', 'decoder', 'layers', '49', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '49', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '49', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '49', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '49', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '49', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '49', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '49', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '49', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '49', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '49', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '49', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '49', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '5', 'fc1', 'bias'), ('model', 'decoder', 'layers', '5', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '5', 'fc2', 'bias'), ('model', 'decoder', 'layers', '5', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '5', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '5', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '5', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '5', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '50', 'fc1', 'bias'), ('model', 'decoder', 'layers', '50', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '50', 'fc2', 'bias'), ('model', 'decoder', 'layers', '50', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '50', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '50', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '50', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '50', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '50', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '50', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '50', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '50', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '50', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '50', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '50', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '50', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '51', 'fc1', 'bias'), ('model', 'decoder', 'layers', '51', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '51', 'fc2', 'bias'), ('model', 'decoder', 'layers', '51', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '51', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '51', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '51', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '51', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '51', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '51', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '51', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '51', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '51', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '51', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '51', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '51', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '52', 'fc1', 'bias'), ('model', 'decoder', 'layers', '52', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '52', 'fc2', 'bias'), ('model', 'decoder', 'layers', '52', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '52', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '52', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '52', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '52', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '52', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '52', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '52', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '52', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '52', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '52', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '52', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '52', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '53', 'fc1', 'bias'), ('model', 'decoder', 'layers', '53', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '53', 'fc2', 'bias'), ('model', 'decoder', 'layers', '53', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '53', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '53', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '53', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '53', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '53', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '53', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '53', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '53', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '53', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '53', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '53', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '53', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '54', 'fc1', 'bias'), ('model', 'decoder', 'layers', '54', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '54', 'fc2', 'bias'), ('model', 'decoder', 'layers', '54', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '54', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '54', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '54', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '54', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '54', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '54', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '54', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '54', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '54', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '54', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '54', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '54', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '55', 'fc1', 'bias'), ('model', 'decoder', 'layers', '55', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '55', 'fc2', 'bias'), ('model', 'decoder', 'layers', '55', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '55', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '55', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '55', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '55', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '55', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '55', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '55', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '55', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '55', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '55', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '55', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '55', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '56', 'fc1', 'bias'), ('model', 'decoder', 'layers', '56', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '56', 'fc2', 'bias'), ('model', 'decoder', 'layers', '56', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '56', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '56', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '56', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '56', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '56', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '56', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '56', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '56', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '56', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '56', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '56', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '56', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '57', 'fc1', 'bias'), ('model', 'decoder', 'layers', '57', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '57', 'fc2', 'bias'), ('model', 'decoder', 'layers', '57', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '57', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '57', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '57', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '57', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '57', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '57', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '57', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '57', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '57', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '57', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '57', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '57', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '58', 'fc1', 'bias'), ('model', 'decoder', 'layers', '58', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '58', 'fc2', 'bias'), ('model', 'decoder', 'layers', '58', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '58', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '58', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '58', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '58', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '58', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '58', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '58', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '58', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '58', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '58', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '58', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '58', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '59', 'fc1', 'bias'), ('model', 'decoder', 'layers', '59', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '59', 'fc2', 'bias'), ('model', 'decoder', 'layers', '59', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '59', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '59', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '59', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '59', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '59', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '59', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '59', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '59', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '59', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '59', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '59', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '59', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '6', 'fc1', 'bias'), ('model', 'decoder', 'layers', '6', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '6', 'fc2', 'bias'), ('model', 'decoder', 'layers', '6', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '6', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '6', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '6', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '6', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '60', 'fc1', 'bias'), ('model', 'decoder', 'layers', '60', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '60', 'fc2', 'bias'), ('model', 'decoder', 'layers', '60', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '60', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '60', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '60', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '60', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '60', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '60', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '60', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '60', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '60', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '60', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '60', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '60', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '61', 'fc1', 'bias'), ('model', 'decoder', 'layers', '61', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '61', 'fc2', 'bias'), ('model', 'decoder', 'layers', '61', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '61', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '61', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '61', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '61', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '61', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '61', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '61', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '61', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '61', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '61', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '61', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '61', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '62', 'fc1', 'bias'), ('model', 'decoder', 'layers', '62', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '62', 'fc2', 'bias'), ('model', 'decoder', 'layers', '62', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '62', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '62', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '62', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '62', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '62', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '62', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '62', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '62', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '62', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '62', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '62', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '62', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '63', 'fc1', 'bias'), ('model', 'decoder', 'layers', '63', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '63', 'fc2', 'bias'), ('model', 'decoder', 'layers', '63', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '63', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '63', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '63', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '63', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '63', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '63', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '63', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '63', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '63', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '63', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '63', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '63', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '7', 'fc1', 'bias'), ('model', 'decoder', 'layers', '7', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '7', 'fc2', 'bias'), ('model', 'decoder', 'layers', '7', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '7', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '7', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '7', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '7', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '8', 'fc1', 'bias'), ('model', 'decoder', 'layers', '8', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '8', 'fc2', 'bias'), ('model', 'decoder', 'layers', '8', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '8', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '8', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '8', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '8', 'self_attn_layer_norm', 'scale'), ('model', 'decoder', 'layers', '9', 'fc1', 'bias'), ('model', 'decoder', 'layers', '9', 'fc1', 'kernel'), ('model', 'decoder', 'layers', '9', 'fc2', 'bias'), ('model', 'decoder', 'layers', '9', 'fc2', 'kernel'), ('model', 'decoder', 'layers', '9', 'final_layer_norm', 'bias'), ('model', 'decoder', 'layers', '9', 'final_layer_norm', 'scale'), ('model', 'decoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('model', 'decoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('model', 'decoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('model', 'decoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('model', 'decoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('model', 'decoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('model', 'decoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('model', 'decoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('model', 'decoder', 'layers', '9', 'self_attn_layer_norm', 'bias'), ('model', 'decoder', 'layers', '9', 'self_attn_layer_norm', 'scale')]\n",
      "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/robv/data/'\n",
    "\n",
    "import datetime\n",
    "import jax\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import FlaxOPTForCausalLM\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.sharding import PositionalSharding\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "MODEL_PATH = \"facebook/opt-66b\"\n",
    "\n",
    "model, params = FlaxOPTForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, dtype=jax.numpy.bfloat16, _do_init=False\n",
    ")\n",
    "params = model.to_bf16(params)\n",
    "\n",
    "print('Params loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices 8\n"
     ]
    }
   ],
   "source": [
    "n_devices = len(jax.devices())\n",
    "print(\"Number of devices\", n_devices)\n",
    "\n",
    "# Use a simple sharding scheme to just fit the model.\n",
    "devices = mesh_utils.create_device_mesh((n_devices, 1))\n",
    "sharding = PositionalSharding(devices)\n",
    "\n",
    "def put_sharded(v):\n",
    "  return jax.device_put(v, sharding.reshape(1, n_devices))\n",
    "\n",
    "# Move model to TPUs \n",
    "\n",
    "params['model']['decoder'] = jax.tree_util.tree_map(\n",
    "    put_sharded, params['model']['decoder']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded AutoTokenizer\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, dtype=jax.numpy.bfloat16)\n",
    "print(\"Loaded AutoTokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length=128\n",
    "output_length=8\n",
    "max_length=136\n",
    "\n",
    "def generator(ids, params,max_length):\n",
    "  return model.generate(\n",
    "      ids, params=params,max_length=136,\n",
    "  )\n",
    "generator_jit = jax.jit(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(10)\n",
    "batch_size = 1\n",
    "\n",
    "tokenizer.vocab_size\n",
    "batch = jax.random.randint(key=key,minval=0,shape=([batch_size,input_length]),maxval=tokenizer.vocab_size-1)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['max_tokens'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mjax\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\n\u001b[1;32m      8\u001b[0m input_ids \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mdevice_put(input_ids, sharding\u001b[39m.\u001b[39mreplicate(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m----> 9\u001b[0m result \u001b[39m=\u001b[39m generator_jit(input_ids, params)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39ms)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresult: \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m.\u001b[39msequences\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m, in \u001b[0;36mgenerator\u001b[0;34m(ids, params)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerator\u001b[39m(ids, params):\n\u001b[0;32m----> 2\u001b[0m   \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      3\u001b[0m       ids, params\u001b[39m=\u001b[39;49mparams,max_tokens\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m\n\u001b[1;32m      4\u001b[0m   )\n",
      "File \u001b[0;32m~/.conda/envs/jax-latest/lib/python3.10/site-packages/transformers/generation/flax_utils.py:330\u001b[0m, in \u001b[0;36mFlaxGenerationMixin.generate\u001b[0;34m(self, input_ids, generation_config, prng_key, trace, params, logits_processor, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m model_kwargs \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m    329\u001b[0m generation_config\u001b[39m.\u001b[39mvalidate()\n\u001b[0;32m--> 330\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[1;32m    332\u001b[0m logits_processor \u001b[39m=\u001b[39m logits_processor \u001b[39mif\u001b[39;00m logits_processor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m FlaxLogitsProcessorList()\n\u001b[1;32m    334\u001b[0m \u001b[39m# set init values\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jax-latest/lib/python3.10/site-packages/transformers/generation/flax_utils.py:262\u001b[0m, in \u001b[0;36mFlaxGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[0;32m--> 262\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    263\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['max_tokens'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Are you conscious?  Please tell me.\"\n",
    ")\n",
    "\n",
    "print(\"Warmup\")\n",
    "s=time.time()\n",
    "#input_ids = tokenizer(prompt, return_tensors=\"jax\").input_ids\n",
    "\n",
    "####\n",
    "\n",
    "input_ids = batch\n",
    "\n",
    "####\n",
    "\n",
    "input_ids = jax.device_put(input_ids, sharding.replicate(axis=0, keepdims=True))\n",
    "result = generator_jit(input_ids, params,max_length)\n",
    "print(time.time()-s)\n",
    "print(f\"result: {result.sequences}\")\n",
    "print(\"Warmup done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized inputs, generating..., 2023-07-13 17:30:21.006024\n",
      "gen_text: </s>Are you conscious?  Please tell me.\n",
      "I am conscious.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>, 0:00:00.225199\n"
     ]
    }
   ],
   "source": [
    "#input_ids = tokenizer(prompt, return_tensors=\"jax\").input_ids\n",
    "s = datetime.datetime.now()\n",
    "print(f\"Tokenized inputs, generating..., {s}\")\n",
    "input_ids = batch\n",
    "\n",
    "input_ids = jax.device_put(input_ids, sharding.replicate(axis=0, keepdims=True))\n",
    "# with jax.profiler.trace(\"/tmp/tensorboard\"):\n",
    "result = generator_jit(input_ids, params,max_length)\n",
    "result.sequences.block_until_ready()\n",
    "print(f'After block-ready: {datetime.datetime.now()-s}')\n",
    "gen_text = tokenizer.batch_decode(result.sequences)[0]\n",
    "print(f\"gen_text: {gen_text}, '\\n',{datetime.datetime.now()-s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "import time\n",
    "\n",
    "# Benchmarking params\n",
    "benchmark_name = \"v5litepod-8-bf16-opt66b\"\n",
    "num_batches=10\n",
    "batch_sizes=[1,2]\n",
    "input_lengths=[128]\n",
    "output_lengths=[8]\n",
    "\n",
    "key = random.PRNGKey(25)\n",
    "\n",
    "print('RUN_NAME                 \\t\\tMEAN TIME\\tTOKENS_PER_SECOND\\tMS_PER_SEQ_OUTPUT_TOKEN')\n",
    "print('='*105)\n",
    "for batch_size in batch_sizes:\n",
    "    for input_length in input_lengths:\n",
    "        for output_length in output_lengths:\n",
    "            max_length = input_length+output_length\n",
    "            key = random.split(key)[0]\n",
    "            input_ids = jax.random.randint(key=key,minval=0,shape=([batch_size,input_length]),maxval=tokenizer.vocab_size-1)\n",
    "            for i in range(num_batches):\n",
    "                start_time = time.time()\n",
    "                input_ids = jax.device_put(input_ids, sharding.replicate(axis=0, keepdims=True))\n",
    "                result = generator_jit(input_ids, params,max_length)\n",
    "                result.sequences.block_until_ready()\n",
    "\n",
    "            mean_time = (time.time() - start_time) / num_batches\n",
    "\n",
    "            num_output_tokens = output_length * batch_size\n",
    "            tokens_per_second = num_output_tokens / mean_time\n",
    "            ms_per_seq_output_token = mean_time * 1000 / output_length\n",
    "\n",
    "            run_name = f'{benchmark_name}_{batch_size}_{input_length}_{output_length}'\n",
    "            print(\n",
    "                f'{run_name}\\t\\t{mean_time:.3f}\\t\\t{tokens_per_second:.3f}\\t\\t\\t{ms_per_seq_output_token:.3f}'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "<>:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "<>:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "<>:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n",
      "/tmp/ipykernel_1122832/4184978536.py:1: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  ['model']['decoder']['embed_positions']['embedding'] = jax.device_put(['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdecoder\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39membed_positions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mdevice_put([\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mdecoder\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39membed_positions\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m], sharding\u001b[39m.\u001b[39mreplicate(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(MODEL_PATH, dtype\u001b[39m=\u001b[39mjax\u001b[39m.\u001b[39mnumpy\u001b[39m.\u001b[39mbfloat16)\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerator\u001b[39m(ids, params):\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def put_sharded(v):\n",
    "  return jax.device_put(v, sharding.reshape(1, n_devices))\n",
    "\n",
    "print(json.dumps(params,indent=2,default=str))\n",
    "\n",
    "['model']['decoder']['embed_positions']['embedding'] = jax.device_put(\n",
    "  ['model']['decoder']['embed_positions']['embedding'], sharding.replicate(axis=0, keepdims=True)\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, dtype=jax.numpy.bfloat16)\n",
    "\n",
    "def generator(ids, params):\n",
    "  return model.generate(\n",
    "      ids, max_length=46, params=params\n",
    "  )\n",
    "\n",
    "generator_compiled = jax.jit(generator)\n",
    "\n",
    "prompt = (\n",
    "    \"In a shocking finding, scientists discovered\"\n",
    "    \" a herd of unicorns living in a remote, \"\n",
    "    \"previously unexplored valley, in the Andes Mountains.\"\n",
    "    \" Even more surprising to the \"\n",
    "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
    ")\n",
    "\n",
    "print(\"Warmup\")\n",
    "s=time.time()\n",
    "input_ids = tokenizer(prompt, return_tensors=\"jax\").input_ids\n",
    "input_ids = jax.device_put(input_ids, sharding.replicate(axis=0, keepdims=True))\n",
    "result = generator_compiled(input_ids, params)\n",
    "# print(time.time()-s)\n",
    "# print(f\"result: {result.sequences}\")\n",
    "# print(\"Warmup done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-latest",
   "language": "python",
   "name": "jax-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
