{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05401b7-190a-430a-9ff7-dba69d1b3d51",
   "metadata": {},
   "source": [
    "### Whisper Inference on Vertex AI\n",
    "The following notebook demostrates running Automatic Speech Recognition via the HuggingFace Transformers library on Vertex AI Prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db12bd-eddb-4d14-bccd-bfef0ce6c55c",
   "metadata": {
    "tags": []
   },
   "source": [
    "A selection of amicorpus mixed headset audio files were downloaded and then copied to cloud storage.  These audio files contain mixed audio from various AMI meetings ranging from 10 min to over an hour.\n",
    "\n",
    "See [here](https://groups.inf.ed.ac.uk/ami/download/) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dbb0835a-a643-4245-9b6c-6ef9eb0022ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.10.1)\n",
      "Collecting torch==1.13.*\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m530.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-0.13.1-cp37-cp37m-manylinux1_x86_64.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.13.*) (4.5.0)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.*) (67.6.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.*) (0.38.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchaudio\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1 torchaudio-0.13.1\n"
     ]
    }
   ],
   "source": [
    "# Pre-reqs\n",
    "import sys\n",
    "!{sys.executable} -m pip install transformers datasets torch==1.13.* torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed38ce83-c446-471f-bb72-8c5ac6b8f7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘CPR’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir CPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8419058-28a5-4b86-87de-cc599f3738c6",
   "metadata": {},
   "source": [
    "### Create Custom Prediction Routine\n",
    "Next, we will create a custom prediction routine using the HuggingFace ASR Pipeline libraries to take in a GCS path to an audio file and render a transcription either inline or back out to a file in cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99c53240-d228-4638-ad4b-a81c6c25647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CPR/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CPR/predictor.py\n",
    "\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "from google.cloud import storage\n",
    "\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset, Audio, load_dataset\n",
    "\n",
    "from pathlib import Path\n",
    "class CprPredictor(Predictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Bring in libsndfile1 if it is not there\n",
    "        import subprocess\n",
    "        subprocess.run([\"apt update && apt install libsndfile1 -y\"], shell=True,capture_output=True)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def _decompose_remote_file(self,remote_file) -> dict:\n",
    "        return {\n",
    "            'filename': remote_file.split('/')[-1],\n",
    "            'bucket': remote_file.split('/')[2],\n",
    "            'file_path': '/'.join(remote_file.split('/')[3:-1])\n",
    "        }\n",
    "\n",
    "    def _map_gcs_file_to_local_file(self,local_dir,remote_file) -> str:\n",
    "        gcs_file_dict = self._decompose_remote_file(remote_file)\n",
    "        return f\"{local_dir}/{gcs_file_dict['bucket']}/{gcs_file_dict['file_path']}/{gcs_file_dict['filename']}\"\n",
    "\n",
    "    def _get_transcription(self, file_path):\n",
    "        return self._transcriber(file_path)\n",
    "    \n",
    "    def _upload_transcription(self, file,contents):\n",
    "        \n",
    "        t = contents['text']\n",
    "        \n",
    "        gcs_file_dict = self._decompose_remote_file(file)\n",
    "        output_filename = f\"{gcs_file_dict['filename']}.txt\"\n",
    "\n",
    "        bucket = self._storage_client.bucket(gcs_file_dict['bucket'])\n",
    "        blob = bucket.blob(f\"{gcs_file_dict['file_path']}/{output_filename}\")\n",
    "        blob.upload_from_string(t)\n",
    "        return f\"gs://{gcs_file_dict['bucket']}/{gcs_file_dict['file_path']}/{output_filename}\"\n",
    "    \n",
    "    def _download_file_from_gcs(self, remote_file) -> str:\n",
    "        local_dir = '/tmp'\n",
    "        gcs_file_dict = self._decompose_remote_file(remote_file)\n",
    "        \n",
    "        # Create local\n",
    "        p = Path(f\"{local_dir}/{gcs_file_dict['bucket']}/{gcs_file_dict['file_path']}\")\n",
    "        print('Creating local dir',p)\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        local_file = self._map_gcs_file_to_local_file('/tmp',remote_file)\n",
    "        \n",
    "        bucket = self._storage_client.bucket(gcs_file_dict['bucket'])\n",
    "        blob = bucket.blob(f\"{gcs_file_dict['file_path']}/{gcs_file_dict['filename']}\")\n",
    "        blob.download_to_filename(local_file)\n",
    "        return local_file        \n",
    "    \n",
    "    def load(self, artifacts_uri: str) -> None:\n",
    "        self._model_name = 'openai/whisper-base'\n",
    "        self._storage_client = storage.Client()\n",
    "        self._transcriber = pipeline(\n",
    "            model=self._model_name,\n",
    "            chunk_length_s=30, \n",
    "            stride_length_s=(5,5), \n",
    "            device=0,\n",
    "            #return_timestamps=True\n",
    "        )\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def preprocess(self, prediction_input: [str]) -> Dataset:\n",
    "        \n",
    "        instances_ds = {}\n",
    "        local_files,source_files, output_formats = [],[],[]\n",
    "        \n",
    "        for instance in prediction_input['instances']:\n",
    "            #Convert to GCS URL for non-tf-model\n",
    "            #instance['input_file'] = instance['input_file'].replace('gs://','https://storage.cloud.google.com/')\n",
    "            \n",
    "            output_format = \"inline\"\n",
    "            \n",
    "            if 'output_format' in instance:\n",
    "                output_format = instance['output_format']\n",
    "                \n",
    "            local_file = self._download_file_from_gcs(instance['input_file'])\n",
    "\n",
    "            local_files.append(local_file)\n",
    "            source_files.append(instance['input_file'])\n",
    "            output_formats.append(output_format)\n",
    "        \n",
    "        instances_ds = {\"audio\":local_files,\"source_file\":source_files,\"output_format\":output_formats }\n",
    "        print(instances_ds)\n",
    "        \n",
    "        #aDataset = load_dataset(\"audiofolder\",data_files=instances_ds,fs=self._fs)\n",
    "\n",
    "        return Dataset.from_dict(instances_ds).cast_column(\"audio\",Audio(sampling_rate=16000)) #aDataset\n",
    "\n",
    "    def predict(self, audio_dataset):\n",
    "        \"\"\"Performs prediction.\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for file in audio_dataset:\n",
    "            output=self._get_transcription(file['audio']['path'])\n",
    "            if file['output_format'] == 'file':\n",
    "                output = self._upload_transcription(file['source_file'],output)\n",
    "                #output = self._upload_transcription(file['audio']['path'],output)\n",
    "\n",
    "            predictions.append({\"input_file\":file['source_file'],\"output_format\":file['output_format'],\"output\":output})\n",
    "\n",
    "        return {\"predictions\": predictions}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba430e4b-b346-4f6f-ad9e-0375d71f0ecd",
   "metadata": {},
   "source": [
    "### Test Custom Prediction Routine `predictor` locally\n",
    "First, we will import our custom prediction routine into the current kernel and verify it properly handles our instance payload and returns the results we expect\n",
    "\n",
    "Note: when changing the CPR above, reload the kernel to see changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6482bb70-2663-4c78-ac5a-99c609a9800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CPR.predictor import CprPredictor\n",
    "\n",
    "predictor = CprPredictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5560ba7-a147-4724-8aa5-f0f58dedff4b",
   "metadata": {},
   "source": [
    "Define a few test instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22827f78-0a9c-49e4-939a-00371b738697",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = {\"instances\":[\n",
    "                {\"input_file\":\"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav\",\"output_format\":\"inline\"},\n",
    "                {\"input_file\":\"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav\",\"output_format\":\"file\"}\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e7398-79a9-4a59-b57e-24b9cb6b29fe",
   "metadata": {},
   "source": [
    "Next, lets call our `preprocess` and `predict` methods directly to test out our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6cb7d23d-bb3a-4a8d-b5c8-9ec93393de36",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating local dir /tmp/gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio\n",
      "Creating local dir /tmp/gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio\n",
      "{'audio': ['/tmp/gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav', '/tmp/gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav'], 'source_file': ['gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav', 'gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav'], 'output_format': ['inline', 'file']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py-3.9_tf-2.11_aip-1.21/lib/python3.9/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py-3.9_tf-2.11_aip-1.21/lib/python3.9/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'input_file': 'gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav',\n",
       "   'output_format': 'inline',\n",
       "   'output': {'text': \" Like, gosh, she's already produced a PowerPoint. I think it's already back. Okay. Right? Well, this is the kickoff meeting for our project. And this is just what we're going to be doing over the next 25 minutes. So first of all, just to kind of make sure that we all know each other. I'm Laura and I'm the project manager. Do you want to introduce yourself again? I'm David and I'm supposed to be an industrial designer. Okay. I'm Andrew and I'm a marketing expert. I'm Greg and I'm a user interface. Great. So we're designing a new remote control. Oh, I have to record he's here actually. David, Andrea and Craig? And you all arrive don't time. Yes, we designed a new remote control. As you can see, it's supposed to be original trendy and user friendly. So that's kind of our brief, so we're, and so there are three different stages to the design. I'm not really sure what you guys have already received in your emails. What did you get? I just got the project announcement. So we're going to have like individual work and then a meeting about it and repeat that process three times And at this point you get try out the whiteboard over there So you get to draw your favorite animal and sum up your favorite characteristics of it. So who would like to go first? Very good. All right. So. This one here, right? Okay. Very nice. All right, my favorite basically, high priority for any animal for me is that they be willing to take a lot of physical affection from their family. And yeah, they have lots of personality and be fit in robust good health. So this is Blue Bigo. My family's Bigo.o. Lovely. No, my favourite animal would be a monkey. And then the small, cute and furry and when plant needs becomes real, and we'll be up there with them. You can take as long as this is you like because we haven't got an awful lot to this. What we do, we do. Don't feel like you're in a reaction. I actually told you a whole lot more about Beagles. I don't know what mine is. I'm going to have to think of the spot knife Impressionist Is that a whale? Yeah! Well, I don't know, it's just the first animal I can think of at the top of my head. The figure reason is, because I'm allergic to most animals. I'm allergic to animals first, so fish was a natural toy. And I kind of like wheels and come in and go, everything inside. And if I'm didn't know what I'm gonna write about. Superbscatch, by the way. I was gonna choose a dog as well, but I'll just draw a different kind of dog. My favorite animal is my own dog at home. That doesn't really look like him actually He lets me when I can pick actually I See a dog in there. Yeah, that's very good of you. Now I see a rooster. What kind is it? He's a mixture of various things. And what did I put him? That's just a guess that his tail works. He's very friendly and cheery and always pleased to see you and very kind of affectionate. And he's quite wee as well, so he doesn't take up too much space. And he does a funny thing where he chases his tail as well. This is quite amusing. Is he aware that this is his own tail that he's chasing? It is, I think it is. He only does it after he's had his dinner. And it just all of a sudden his get up and start chasing his tail. And he's around delivering him. Probably when he was little he got lots of attention for doing it and has forever been conditioned. Maybe. Where did you find this just down here? Okay. We'll be doing next. Okay, when I need to discuss the project finance. So according to the brief, we're going to be selling this remote control for 25 euro and we're aiming to make 50 million euro. So we're going to be selling this one in international scale and We don't want it to cost any more than 1250 euros so 50% of the selling price Can we just go over that again sure? So this all right, yeah So cost like production cost is all together 50 but selling prices is that wholesale or retail like on the shelf? I don't know I imagine that's a good question I imagine it probably is or sale actually because it's probably up to the retailer to sell it for whatever price they want. But I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all? Yes. I think it will. Well right away I'm wondering if there's like with DVD players if there are zones. Oh yeah, regions and stuff. Frequency or something. Yeah. Okay. As well as characters, different keypad styles and symbols. Yeah, but for a remote control, do you think that would be as business depends on how complicated our remote control is? Yeah, yeah, yeah, okay, and then and then the other thing international on the topic of price I'm thinking The price might might appeal to a certain market in one region whereas in another it'll be different So it just I can turn just a care to the country like how much money people have to spend on things like basic product positioning the 25 euro remote control might be a big hit in London might not be such a big hit in Greece Good marketing thoughts. Oh gosh, I should be writing all this time. Right away I'm making some kind of assumptions about what information we're given here. Thinking, okay, Trendy probably means something other than just basic, something other than just standard. So I'm wondering right away selling 25 years is that sort of the, is this going to be like the premium product? Yeah, yeah, like how much does, you know, a remote control cost? Well 25 euro, I mean, that's, that's about like 18 pounds or something, isn't it? Or is it as much as that 16 17 18 pounds? I don't know I've never bought a remote control so I don't know how I get a remote control that would get you But yeah, I suppose it has to look kind of cool gimmicky. Okay, let me just scoot on the head here. Okay, does anybody have anything to add to the finance issue at all? Do we have any other background information on how that that compares to other? No actually that would be useful though wouldn't it if you knew like what your money would get you? No. Here interesting thing about discussing production of remote control for me is that as you point out I just don't think remote control is being something something people consciously assess in their purchasing habits. It's just like Getting shoelaces with shoes or something just five minutes to end of meeting. Oh, okay. We're a bit behind. You know what I mean like? Yeah, so sort of like how do you I mean one one way look at it would be well the people producing Television sets maybe they buy remote controls. Or in other ways, maybe people who have TV sets are really fed up with their remote control. And they really want a better one or something. I'm not here in 2021, but I'm more controlled because they got fed up by having four or five different remote controls for each time. Right. Right. And it was just how many devices is a control? Right. So in function, one of the priorities might be to combine as many uses. Right. So do you think that should be like a man designing a form of control? Do you know? Do your satellite and your regular tally and your VCR and everything? Well, like maybe what we could use as a sort of like an example of a successful other piece of technology is palm pilots. They're going from being just little scribble boards to cameras, MP3 players, telephones, everything, agenda. So I wonder if we might add something new to the remote control market, such as the lighting in your house. Or even like you need notes about what you want to watch like you might put in there or I want to watch and such and look at that's a good idea and so extra functionalities like personally for me at home I've I've combined the audio video of my television set and my DVD player and my CD player so they all work actually function together but I have different remote controls for each of them. So it's sort of ironic that in their, you know, the sound and everything, it's just one system, but each one's got its own little part. Okay. I'm gonna have to wrap up pretty quickly in the next couple of minutes. I'll just check with nothing else. Okay, so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all? You keep losing them. You keep losing them, okay. You get these ones where you can if you like whistle or make really high pitched noise they beep. I mean is that something we'd want to include do you think? I don don't know. Sure. OK, maybe. I remember when the first remote control my family had was on a cable. Actually, the cable between TV and big buttons that sort of like on a blender or something. My goodness. And you know, and I think about what they are now is better, but actually it's still kind of, I don't know, like a massive junky thing on the table. Maybe we could think about how could be more, you know, streamline. Maybe like touch screen or something. Something like that or whatever would be technologically reasonable. It could be that functionally that doesn't make it any better, but that just the appeal of not having, you know, these days, days every Putes things in people's homes are becoming more and more like chic, you know nicer materials and okay might be Okay, be worth exploring right well, um, so it is to wrap up the next meeting is gonna be in 30 minutes, so that's about um about 10 to 12 by my watch. So in between nine then, as the industrial designer, you're going to be working on, you know, that's a working design of it, so you know what you're doing there. For our user interface, technical functions, I guess that's, you know, like what we've been talking about, what it'll actually do. Marketing executive, you'll be just thinking about what it actually, what requirements it has to fulfill. And you'll get instructions emailed to you, I guess. Okay. Okay. Yes, so it's the functional design stage is next, I guess. And that's the end of the meeting. I got that little message. I locked the scene as an eye thought I would. Before we we wrap up just to make sure we're on the same page here. We're given sort of an example of a coffee machine or something. Are we right now on the assumption that our television remote control may have features which go beyond the television? Or are we keeping sort of like a design commitment to television features? Okay, well just very quickly because it's supposed to finish. No, I guess that's up to us. I mean you probably want some kind of unique selling point of it. So, you know. The factor would be production cost. Yeah. It depends on how much you can cram into that price. Yeah. Okay. Rashed, okay. Well, that's the end of the meeting then. Thank you all for coming. Could I hope that was what they wanted us to do, isn't it? Right? Great. How do you turn this thing off? Is it function and... Oh, there we go. I think if you can just leave it on maybe and then... Oh god, it is. This is all very high-tech, I must say. I don't have a chapter. I'm talking about the SDP. Computer design, all-terrain systems, and the idea. Next semester? Oh, next semester. Computer security. Computer architecture. I don't know. Sorry. I don't know. I don't know. I mean, I don't expecting the five minutes\"}},\n",
       "  {'input_file': 'gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav',\n",
       "   'output_format': 'file',\n",
       "   'output': 'gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav.txt'}]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predictor.predict(predictor.preprocess(instances))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f6908-ad9f-4cb8-ba2f-31dfaff73d1d",
   "metadata": {},
   "source": [
    "For our test instances, we specified to return the response inline for the first input file, and to write the results back to Cloud Storage for the second file.  Let's quickly check the output of the second file, just grabbing the first 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "76ec3f74-62f1-4a83-b0c5-ddce346bd8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file path: gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav.txt\n",
      " Oh yes, I forgot about that. Okay. So, right now. Okay. That's cool. Someone turn these on. Sorry? Did someone turn these on for us? Great. Okay, everybody, I'll start the meeting. Okay. We've got half an hour for this one to discuss the functional design Thanks Already to go okay, okay, so hopefully if I was in working away and I've put the minutes of the last meeting in the project folder. So I guess just to recap on what we did last time. I got to know each other a little bit and got familiar with all the equipment and started to discuss a bit about the project, you know, cost wise, how much money we have three new requirements, which is the first one is that The companies decided that teletext is outdated because of how popular the internet is nobody uses teletext very much anymore So we don't really need to consider that in the functionality of the remote control and they've also suggested that we We only use remote control to control the television, not the VCR or DVD or anythin"
     ]
    }
   ],
   "source": [
    "output_file = predictions['predictions'][1]['output']\n",
    "print(f'Output file path: {output_file}')\n",
    "!gsutil cat $output_file | head -c 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f83d7-c101-462a-874a-fddc0260deb8",
   "metadata": {},
   "source": [
    "### Building the CPR Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1520bb1-085c-4377-937f-99bc916c4ba2",
   "metadata": {},
   "source": [
    "Now that we've tested our CPR predictor routine locally with some sample instances and verified the output, lets build the CPR container, test the container locally, and then make it available in Vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21792e-0973-423d-8984-691fc0e05b5d",
   "metadata": {},
   "source": [
    "First, let's capture our library requirements for installation into our CPR prediction container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bee1eaa1-2d64-4dd5-b5b1-2c8e267b02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CPR/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile CPR/requirements.txt\n",
    "datasets==2.10.0\n",
    "google-cloud-storage==2.7.0\n",
    "transformers==4.27.3\n",
    "torchaudio==0.13.*\n",
    "librosa==0.10.0\n",
    "gcsfs==2023.1.0\n",
    "protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc82168-c88c-4dd1-b0bc-b081125725ab",
   "metadata": {},
   "source": [
    "Double-checking the requirements file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b2c7b93-7f0a-4363-9765-ff33dc979808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets==2.10.0\n",
      "google-cloud-storage==2.7.0\n",
      "transformers==4.27.3\n",
      "torchaudio==0.13.*\n",
      "librosa==0.10.0\n",
      "gcsfs==2023.1.0\n",
      "protobuf==3.20.*\n"
     ]
    }
   ],
   "source": [
    "cat CPR/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd2484-526f-44ff-b7a6-c9645db2fbad",
   "metadata": {},
   "source": [
    "Next, lets build our CPR predictor image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2947858f-a1e7-41fc-825a-ba8099764abc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.docker_utils.build:Running command: docker build -t us-central1-docker.pkg.dev/gcp-ml-sandbox/vertex-custom-containers/whisper-base-asr-pt-training-gpu --rm -f- /home/jupyter/whisper/notebooks\n",
      "/opt/conda/envs/py-3.9_tf-2.11_aip-1.21/lib/python3.9/subprocess.py:935: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/conda/envs/py-3.9_tf-2.11_aip-1.21/lib/python3.9/subprocess.py:941: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Sending build context to Docker daemon  7.448MB\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 1/14 : FROM pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> 71eb2d092138\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 2/14 : ENV PYTHONDONTWRITEBYTECODE=1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> 2f5dc93edcf2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 3/14 : EXPOSE 8080\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> 13fb372bece9\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 4/14 : ENTRYPOINT [\"python\", \"-m\", \"google.cloud.aiplatform.prediction.model_server\"]\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> 3457884a618a\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 5/14 : RUN mkdir -m 777 -p /usr/app /home\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> e675349431ef\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 6/14 : WORKDIR /usr/app\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> d5fca4d087b5\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 7/14 : ENV HOME=/home\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> 84ee8510f1cd\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 8/14 : RUN pip install --no-cache-dir --force-reinstall 'google-cloud-aiplatform[prediction]>=1.16.0'\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> 76e87b52913f\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 9/14 : ENV HANDLER_MODULE=google.cloud.aiplatform.prediction.handler\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> 19ef0c34b849\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 10/14 : ENV HANDLER_CLASS=PredictionHandler\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> 3f1de3f253a1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 11/14 : ENV PREDICTOR_MODULE=CPR.predictor\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> 07c4022eca31\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 12/14 : ENV PREDICTOR_CLASS=CprPredictor\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Using cache\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> df0e108725be\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 13/14 : COPY [\".\", \".\"]\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> 6f2f0eeb3359\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Step 14/14 : RUN pip install --no-cache-dir --force-reinstall -r CPR/requirements.txt\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> Running in 36b283256ef6\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting datasets==2.10.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading datasets-2.10.0-py3-none-any.whl (469 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 11.7 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting google-cloud-storage==2.7.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.2/110.2 kB 153.7 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting transformers==4.27.3\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 144.8 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting torchaudio==0.13.*\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 278.8 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting librosa==0.10.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 252.9/252.9 kB 222.6 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting gcsfs==2023.1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading gcsfs-2023.1.0-py2.py3-none-any.whl (26 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting protobuf==3.20.*\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 301.5 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting pandas\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 162.4 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting pyyaml>=5.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 682.2/682.2 kB 275.3 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting dill<0.3.7,>=0.3.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 167.8 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting tqdm>=4.62.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 158.5 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting xxhash\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 211.3 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting packaging\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.7/42.7 kB 117.3 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting fsspec[http]>=2021.11.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145.4/145.4 kB 172.0 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting responses<0.19\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting pyarrow>=6.0.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.9/34.9 MB 202.0 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.8/199.8 kB 203.4 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting aiohttp\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 297.2 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting numpy>=1.17\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 138.7 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting multiprocess\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 169.0 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting requests>=2.19.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 140.2 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting google-resumable-media>=2.3.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 kB 151.3 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting google-auth<3.0dev,>=1.25.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.2/177.2 kB 288.9 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.3/120.3 kB 220.4 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting regex!=2019.12.17\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading regex-2023.3.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 769.6/769.6 kB 311.2 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 60.3 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting filelock\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading filelock-3.10.3-py3-none-any.whl (10 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting torch==1.13.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 887.5/887.5 MB 147.8 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting numba>=0.51.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading numba-0.56.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 194.6 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting scipy>=1.2.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.4/34.4 MB 154.2 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting joblib>=0.14\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 291.6 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting typing-extensions>=4.1.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting audioread>=2.1.9\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 377.0/377.0 kB 302.5 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Preparing metadata (setup.py): started\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Preparing metadata (setup.py): finished with status 'done'\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting decorator>=4.3.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting msgpack>=1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading msgpack-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.8/316.8 kB 299.0 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting soundfile>=0.12.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading soundfile-0.12.1-py2.py3-none-any.whl (24 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting soxr>=0.3.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading soxr-0.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 292.7 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting lazy-loader>=0.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading lazy_loader-0.2-py3-none-any.whl (8.6 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting pooch>=1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading pooch-1.7.0-py3-none-any.whl (60 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.9/60.9 kB 212.5 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting scikit-learn>=0.20.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 158.8 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting fsspec==2023.1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.0/143.0 kB 235.8 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting google-auth-oauthlib\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 199.2 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 180.6 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 190.7 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 321.8 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting wheel\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.5/64.5 kB 184.5 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting setuptools\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading setuptools-67.6.0-py3-none-any.whl (1.1 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 317.0 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting multidict<7.0,>=4.5\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 264.4 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting aiosignal>=1.1.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting yarl<2.0,>=1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading yarl-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.0/264.0 kB 285.0 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting attrs>=17.3.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 kB 134.1 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting charset-normalizer<4.0,>=2.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.3/199.3 kB 169.4 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting frozenlist>=1.1.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 176.9 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting async-timeout<5.0,>=4.0.0a3\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 223.6/223.6 kB 200.4 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting pyasn1-modules>=0.2.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 182.7 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting six>=1.9.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting cachetools<6.0,>=2.0.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting rsa<5,>=3.1.4\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting google-crc32c<2.0dev,>=1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting numpy>=1.17\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 127.5 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading llvmlite-0.39.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.6/34.6 MB 253.3 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting platformdirs>=2.5.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading platformdirs-3.1.1-py3-none-any.whl (14 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting urllib3<1.27,>=1.21.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.9/140.9 kB 177.4 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting certifi>=2017.4.17\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 261.3 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting idna<4,>=2.5\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 210.2 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting threadpoolctl>=2.0.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting cffi>=1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 441.8/441.8 kB 207.2 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting requests-oauthlib>=0.7.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting pytz>=2020.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 499.4/499.4 kB 223.9 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting python-dateutil>=2.8.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 234.2 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting pycparser\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.7/118.7 kB 250.7 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting pyasn1<0.5.0,>=0.4.6\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 229.2 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Collecting oauthlib>=3.0.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 168.7 MB/s eta 0:00:00\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Building wheels for collected packages: audioread\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Building wheel for audioread (setup.py): started\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Building wheel for audioread (setup.py): finished with status 'done'\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23703 sha256=5f81da8d83591f6d68b5559f4d25f68af72fd6d7167c0a03eb099decdc6ad5c2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Stored in directory: /tmp/pip-ephem-wheel-cache-wg2eq6st/wheels/3d/e0/4b/51b8aa4a0e2b6361c14943f7a7c2c7adb7b9cd419e0a1b720d\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Successfully built audioread\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Installing collected packages: tokenizers, pytz, pyasn1, msgpack, xxhash, wheel, urllib3, typing-extensions, tqdm, threadpoolctl, six, setuptools, rsa, regex, pyyaml, pycparser, pyasn1-modules, protobuf, platformdirs, packaging, oauthlib, nvidia-cuda-nvrtc-cu11, numpy, multidict, llvmlite, lazy-loader, joblib, idna, google-crc32c, fsspec, frozenlist, filelock, dill, decorator, charset-normalizer, certifi, cachetools, audioread, attrs, async-timeout, yarl, soxr, scipy, requests, python-dateutil, pyarrow, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, numba, multiprocess, googleapis-common-protos, google-resumable-media, google-auth, cffi, aiosignal, soundfile, scikit-learn, responses, requests-oauthlib, pooch, pandas, nvidia-cudnn-cu11, huggingface-hub, google-api-core, aiohttp, transformers, torch, librosa, google-cloud-core, google-auth-oauthlib, torchaudio, google-cloud-storage, datasets, gcsfs\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: pytz\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: pytz 2022.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling pytz-2022.1:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled pytz-2022.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: pyasn1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: pyasn1 0.4.8\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling pyasn1-0.4.8:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled pyasn1-0.4.8\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: wheel\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: wheel 0.37.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling wheel-0.37.1:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled wheel-0.37.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: urllib3\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: urllib3 1.26.15\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling urllib3-1.26.15:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled urllib3-1.26.15\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: typing-extensions\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: typing_extensions 4.5.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling typing_extensions-4.5.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled typing_extensions-4.5.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: tqdm\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: tqdm 4.64.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling tqdm-4.64.1:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled tqdm-4.64.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: six\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: six 1.16.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling six-1.16.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled six-1.16.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: setuptools\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: setuptools 65.5.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling setuptools-65.5.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled setuptools-65.5.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: rsa\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: rsa 4.9\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling rsa-4.9:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled rsa-4.9\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: pyyaml\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: PyYAML 6.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling PyYAML-6.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled PyYAML-6.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: pycparser\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: pycparser 2.21\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling pycparser-2.21:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled pycparser-2.21\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: pyasn1-modules\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: pyasn1-modules 0.2.8\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling pyasn1-modules-0.2.8:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled pyasn1-modules-0.2.8\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: protobuf\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: protobuf 4.22.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling protobuf-4.22.1:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled protobuf-4.22.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: packaging\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: packaging 21.3\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling packaging-21.3:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled packaging-21.3\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: numpy\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: numpy 1.22.3\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling numpy-1.22.3:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled numpy-1.22.3\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: idna\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: idna 3.4\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling idna-3.4:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled idna-3.4\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: google-crc32c\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: google-crc32c 1.5.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling google-crc32c-1.5.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled google-crc32c-1.5.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: filelock\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: filelock 3.6.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling filelock-3.6.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled filelock-3.6.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: decorator\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: decorator 5.1.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling decorator-5.1.1:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled decorator-5.1.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: charset-normalizer\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: charset-normalizer 3.1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling charset-normalizer-3.1.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled charset-normalizer-3.1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: certifi\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: certifi 2022.12.7\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling certifi-2022.12.7:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled certifi-2022.12.7\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: cachetools\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: cachetools 5.3.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling cachetools-5.3.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled cachetools-5.3.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: attrs\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: attrs 22.1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling attrs-22.1.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled attrs-22.1.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: requests\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: requests 2.28.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling requests-2.28.2:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled requests-2.28.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: python-dateutil\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: python-dateutil 2.8.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling python-dateutil-2.8.2:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled python-dateutil-2.8.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: googleapis-common-protos\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: googleapis-common-protos 1.59.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling googleapis-common-protos-1.59.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled googleapis-common-protos-1.59.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: google-resumable-media\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: google-resumable-media 2.4.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling google-resumable-media-2.4.1:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled google-resumable-media-2.4.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: google-auth\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: google-auth 2.16.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling google-auth-2.16.2:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled google-auth-2.16.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: cffi\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: cffi 1.15.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling cffi-1.15.1:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled cffi-1.15.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: google-api-core\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: google-api-core 2.11.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling google-api-core-2.11.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled google-api-core-2.11.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: torch\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: torch 1.13.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling torch-1.13.1:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled torch-1.13.1\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: google-cloud-core\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: google-cloud-core 2.3.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling google-cloud-core-2.3.2:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled google-cloud-core-2.3.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:  Attempting uninstall: google-cloud-storage\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Found existing installation: google-cloud-storage 2.7.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:    Uninstalling google-cloud-storage-2.7.0:\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:      Successfully uninstalled google-cloud-storage-2.7.0\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:grpcio-status 1.51.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:google-cloud-aiplatform 1.23.0 requires packaging<22.0.0dev,>=14.3, but you have packaging 23.0 which is incompatible.\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-22.2.0 audioread-3.0.0 cachetools-5.3.0 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-3.1.0 datasets-2.10.0 decorator-5.1.1 dill-0.3.6 filelock-3.10.3 frozenlist-1.3.3 fsspec-2023.1.0 gcsfs-2023.1.0 google-api-core-2.11.0 google-auth-2.16.2 google-auth-oauthlib-1.0.0 google-cloud-core-2.3.2 google-cloud-storage-2.7.0 google-crc32c-1.5.0 google-resumable-media-2.4.1 googleapis-common-protos-1.59.0 huggingface-hub-0.13.3 idna-3.4 joblib-1.2.0 lazy-loader-0.2 librosa-0.10.0 llvmlite-0.39.1 msgpack-1.0.5 multidict-6.0.4 multiprocess-0.70.14 numba-0.56.4 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 oauthlib-3.2.2 packaging-23.0 pandas-1.5.3 platformdirs-3.1.1 pooch-1.7.0 protobuf-3.20.3 pyarrow-11.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 python-dateutil-2.8.2 pytz-2022.7.1 pyyaml-6.0 regex-2023.3.23 requests-2.28.2 requests-oauthlib-1.3.1 responses-0.18.0 rsa-4.9 scikit-learn-1.2.2 scipy-1.10.1 setuptools-67.6.0 six-1.16.0 soundfile-0.12.1 soxr-0.3.4 threadpoolctl-3.1.0 tokenizers-0.13.2 torch-1.13.1 torchaudio-0.13.1 tqdm-4.65.0 transformers-4.27.3 typing-extensions-4.5.0 urllib3-1.26.15 wheel-0.40.0 xxhash-3.2.0 yarl-1.8.2\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:\u001b[0mRemoving intermediate container 36b283256ef6\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util: ---> f4b0104321b7\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Successfully built f4b0104321b7\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Successfully tagged us-central1-docker.pkg.dev/gcp-ml-sandbox/vertex-custom-containers/whisper-base-asr-pt-training-gpu:latest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "from CPR.predictor import CprPredictor\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import os\n",
    "\n",
    "# {import your predictor and handler}\n",
    "REGION='us-central1'\n",
    "PROJECT_ID='gcp-ml-sandbox'\n",
    "REPOSITORY='vertex-custom-containers'\n",
    "IMAGE='whisper-base-asr-pt-training-gpu'\n",
    "BASE_IMAGE='pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime'\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    os.getcwd(),\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\",\n",
    "    predictor=CprPredictor,\n",
    "    base_image=BASE_IMAGE,\n",
    "    requirements_path='CPR/requirements.txt',\n",
    "    no_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffcc561-f159-46bf-b2ab-274e49675fff",
   "metadata": {},
   "source": [
    "With our local model built, lets see what our `serving_container_specs` look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b1457e48-6810-4de8-8d92-4c212670f2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_uri: \"us-central1-docker.pkg.dev/gcp-ml-sandbox/vertex-custom-containers/whisper-base-asr-pt-training-gpu\"\n",
       "predict_route: \"/predict\"\n",
       "health_route: \"/health\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_specs = local_model.get_serving_container_spec()\n",
    "serving_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb4ae7-bd0f-4cba-9ee8-a7b7af7d0559",
   "metadata": {},
   "source": [
    "In order to reduce contention with accessing the GPU, lets pin the workers to 1.  See the [build_cpr_method](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.prediction.LocalModel#google_cloud_aiplatform_prediction_LocalModel_build_cpr_model) method for more info on controlling the # of workers in a custom prediction routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f81abd9f-7594-46fe-8a2f-a2f0b480ac2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_uri: \"us-central1-docker.pkg.dev/gcp-ml-sandbox/vertex-custom-containers/whisper-base-asr-pt-training-gpu\"\n",
       "env {\n",
       "  name: \"VERTEX_CPR_MAX_WORKERS\"\n",
       "  value: \"1\"\n",
       "}\n",
       "predict_route: \"/predict\"\n",
       "health_route: \"/health\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_specs = local_model.get_serving_container_spec()\n",
    "\n",
    "serving_specs.env = [{'name':\"VERTEX_CPR_MAX_WORKERS\",'value':\"1\"}]\n",
    "local_model.serving_container_spec = serving_specs\n",
    "\n",
    "local_model.get_serving_container_spec()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62015881-7ec6-4b0c-b5db-bf6a1829cceb",
   "metadata": {},
   "source": [
    "Next, lets test out our model locally by deploying it to a local endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7cd22e1-a2a4-43a5-9a98-432ae4e7e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.prediction.local_endpoint:Got the project id from the global config: gcp-ml-sandbox.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> b'{}'\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform.prediction import LocalModel,LocalEndpoint\n",
    "\n",
    "with local_model.deploy_to_local_endpoint(\n",
    "    host_port=8081,\n",
    "    gpu_count=-1\n",
    ") as local_endpoint:\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    print(health_check_response, health_check_response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "22253724-9385-46d5-a585-96e0a60d0875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.cloud.aiplatform.prediction.local_endpoint:The local endpoint has started serving traffic. No need to call `serve()` again.\n"
     ]
    }
   ],
   "source": [
    "# Start our server backup\n",
    "local_endpoint.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99fe4032-3004-4de7-a040-ccaa628699ed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py-3.9_tf-2.11_aip-1.21/lib/python3.9/subprocess.py:935: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/conda/envs/py-3.9_tf-2.11_aip-1.21/lib/python3.9/subprocess.py:941: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Using default tag: latest\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:The push refers to repository [us-central1-docker.pkg.dev/gcp-ml-sandbox/vertex-custom-containers/whisper-base-stt-tf-training-gpu]\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:505f42865e39: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:daf16329ed27: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:9cfdcbb035f2: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:c15df39cb355: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:e42695c7b436: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:e42695c7b436: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:72f0f663075e: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:0c307ceb7de8: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:222114f9d6e2: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:a92ff333694c: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:a92ff333694c: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:7c29e0265c37: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:11b2f35a73b5: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:085b36bb4db8: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:022bf439e63b: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:5173e5d8a35c: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:2388f39f222d: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:aa04497f1d2b: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:aa04497f1d2b: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:73d31b8268a9: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:919f56b388a1: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:656d90f81317: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:ff6785a3ad77: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:dd9eafec821e: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:338fa05f2866: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:a06b969a2152: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:f857cc25e56d: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:82471d6461d7: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:5ac9d337aafc: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:a735f6dad37e: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:bc6af4d104d8: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:aa25d86089a5: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:4cbfec24b842: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:8d2278f0bee2: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:2814e52cd888: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:ee9a177ae05b: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:339bec97e3ba: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:55df74fa2018: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:89c3a55cc66f: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:532c075aae0f: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:af18a29a82ba: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:d8f3347e3ae5: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:72f0f663075e: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3663689faddd: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:5f70bf18a086: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3bec1a9c1f4d: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:0c307ceb7de8: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:60766204fa42: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:b3936e4c67d2: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:07d37209b7a9: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:fed0e8aa65b5: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:a92ff333694c: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:222114f9d6e2: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:ad8fec0b36f1: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3a217af3edf9: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:7c29e0265c37: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3297f5de02be: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:11b2f35a73b5: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:085b36bb4db8: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:f3717d7fdfb7: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:e1eace4c0976: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:2388f39f222d: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:022bf439e63b: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:8d2278f0bee2: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:959a7375cb04: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:5173e5d8a35c: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:aa04497f1d2b: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:89c3a55cc66f: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:d79c672e1e8b: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:2814e52cd888: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:73d31b8268a9: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:339bec97e3ba: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:d8f3347e3ae5: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:532c075aae0f: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:7b7c9e761223: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:af18a29a82ba: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:ee9a177ae05b: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:0002c93bdb37: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:919f56b388a1: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:82471d6461d7: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:55df74fa2018: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3663689faddd: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:5ac9d337aafc: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:5f70bf18a086: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:338fa05f2866: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3bec1a9c1f4d: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:a06b969a2152: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:60766204fa42: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:a735f6dad37e: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:4cbfec24b842: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:f857cc25e56d: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:e1eace4c0976: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:bc6af4d104d8: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:7b7c9e761223: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:b3936e4c67d2: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:959a7375cb04: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:aa25d86089a5: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:0002c93bdb37: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3a217af3edf9: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:d79c672e1e8b: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:f3717d7fdfb7: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:dd9eafec821e: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:07d37209b7a9: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:ad8fec0b36f1: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:fed0e8aa65b5: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:656d90f81317: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:c15df39cb355: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:9cfdcbb035f2: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:e42695c7b436: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:0c307ceb7de8: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:72f0f663075e: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:222114f9d6e2: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:a92ff333694c: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:7c29e0265c37: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:085b36bb4db8: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:022bf439e63b: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:5173e5d8a35c: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:11b2f35a73b5: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:2388f39f222d: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:73d31b8268a9: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:aa04497f1d2b: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:919f56b388a1: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:656d90f81317: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:ff6785a3ad77: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:dd9eafec821e: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:338fa05f2866: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:a06b969a2152: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:f857cc25e56d: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:82471d6461d7: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:5ac9d337aafc: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:a735f6dad37e: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:bc6af4d104d8: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:daf16329ed27: Pushed\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:aa25d86089a5: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:4cbfec24b842: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:ee9a177ae05b: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:8d2278f0bee2: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:2814e52cd888: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:339bec97e3ba: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:89c3a55cc66f: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:55df74fa2018: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:532c075aae0f: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:5f70bf18a086: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:d8f3347e3ae5: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:af18a29a82ba: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3663689faddd: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3bec1a9c1f4d: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:60766204fa42: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:b3936e4c67d2: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:07d37209b7a9: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:fed0e8aa65b5: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:ad8fec0b36f1: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3a217af3edf9: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:3297f5de02be: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:e1eace4c0976: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:f3717d7fdfb7: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:959a7375cb04: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:7b7c9e761223: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:d79c672e1e8b: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:0002c93bdb37: Layer already exists\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:505f42865e39: Pushed\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:latest: digest: sha256:7181aff851ca9b73d96774d680a37e10e328ecc17d5c843a491906f9ba51c346 size: 12466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c8376d07-a644-489d-afa7-9f21c559e531",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'input_file': 'gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav',\n",
       "   'output_format': 'inline',\n",
       "   'output': {'text': \" Like, gosh, she's already produced a PowerPoint. I think it's already back. Okay. Right? Well, this is the kickoff meeting for our project. And this is just what we're going to be doing over the next 25 minutes. So first of all, just to kind of make sure that we all know each other. I'm Laura and I'm the project manager. Do you want to introduce yourself again? I'm David and I'm supposed to be an industrial designer. Okay. I'm Andrew and I'm a marketing expert. I'm Greg and I'm a user interface. Great. So we're designing a new remote control. Oh, I have to record he's here actually. David, Andrea and Craig? And you all arrive don't time. Yes, we designed a new remote control. As you can see, it's supposed to be original trendy and user friendly. So that's kind of our brief, so we're, and so there are three different stages to the design. I'm not really sure what you guys have already received in your emails. What did you get? I just got the project announcement. So we're going to have like individual work and then a meeting about it and repeat that process three times And at this point you get try out the whiteboard over there So you get to draw your favorite animal and sum up your favorite characteristics of it. So who would like to go first? Very good. All right. So. This one here, right? Okay. Very nice. All right, my favorite basically, high priority for any animal for me is that they be willing to take a lot of physical affection from their family. And yeah, they have lots of personality and be fit in robust good health. So this is Blue Bigo. My family's Bigo.o. Lovely. No, my favourite animal would be a monkey. And then the small, cute and furry and when plant needs becomes real, and we'll be up there with them. You can take as long as this is you like because we haven't got an awful lot to this. What we do, we do. Don't feel like you're in a reaction. I actually told you a whole lot more about Beagles. I don't know what mine is. I'm going to have to think of the spot knife Impressionist Is that a whale? Yeah! Well, I don't know, it's just the first animal I can think of at the top of my head. The figure reason is, because I'm allergic to most animals. I'm allergic to animals first, so fish was a natural toy. And I kind of like wheels and come in and go, everything inside. And if I'm didn't know what I'm gonna write about. Superbscatch, by the way. I was gonna choose a dog as well, but I'll just draw a different kind of dog. My favorite animal is my own dog at home. That doesn't really look like him actually He lets me when I can pick actually I See a dog in there. Yeah, that's very good of you. Now I see a rooster. What kind is it? He's a mixture of various things. And what did I put him? That's just a guess that his tail works. He's very friendly and cheery and always pleased to see you and very kind of affectionate. And he's quite wee as well, so he doesn't take up too much space. And he does a funny thing where he chases his tail as well. This is quite amusing. Is he aware that this is his own tail that he's chasing? It is, I think it is. He only does it after he's had his dinner. And it just all of a sudden his get up and start chasing his tail. And he's around delivering him. Probably when he was little he got lots of attention for doing it and has forever been conditioned. Maybe. Where did you find this just down here? Okay. We'll be doing next. Okay, when I need to discuss the project finance. So according to the brief, we're going to be selling this remote control for 25 euro and we're aiming to make 50 million euro. So we're going to be selling this one in international scale and We don't want it to cost any more than 1250 euros so 50% of the selling price Can we just go over that again sure? So this all right, yeah So cost like production cost is all together 50 but selling prices is that wholesale or retail like on the shelf? I don't know I imagine that's a good question I imagine it probably is or sale actually because it's probably up to the retailer to sell it for whatever price they want. But I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all? Yes. I think it will. Well right away I'm wondering if there's like with DVD players if there are zones. Oh yeah, regions and stuff. Frequency or something. Yeah. Okay. As well as characters, different keypad styles and symbols. Yeah, but for a remote control, do you think that would be as business depends on how complicated our remote control is? Yeah, yeah, yeah, okay, and then and then the other thing international on the topic of price I'm thinking The price might might appeal to a certain market in one region whereas in another it'll be different So it just I can turn just a care to the country like how much money people have to spend on things like basic product positioning the 25 euro remote control might be a big hit in London might not be such a big hit in Greece Good marketing thoughts. Oh gosh, I should be writing all this time. Right away I'm making some kind of assumptions about what information we're given here. Thinking, okay, Trendy probably means something other than just basic, something other than just standard. So I'm wondering right away selling 25 years is that sort of the, is this going to be like the premium product? Yeah, yeah, like how much does, you know, a remote control cost? Well 25 euro, I mean, that's, that's about like 18 pounds or something, isn't it? Or is it as much as that 16 17 18 pounds? I don't know I've never bought a remote control so I don't know how I get a remote control that would get you But yeah, I suppose it has to look kind of cool gimmicky. Okay, let me just scoot on the head here. Okay, does anybody have anything to add to the finance issue at all? Do we have any other background information on how that that compares to other? No actually that would be useful though wouldn't it if you knew like what your money would get you? No. Here interesting thing about discussing production of remote control for me is that as you point out I just don't think remote control is being something something people consciously assess in their purchasing habits. It's just like Getting shoelaces with shoes or something just five minutes to end of meeting. Oh, okay. We're a bit behind. You know what I mean like? Yeah, so sort of like how do you I mean one one way look at it would be well the people producing Television sets maybe they buy remote controls. Or in other ways, maybe people who have TV sets are really fed up with their remote control. And they really want a better one or something. I'm not here in 2021, but I'm more controlled because they got fed up by having four or five different remote controls for each time. Right. Right. And it was just how many devices is a control? Right. So in function, one of the priorities might be to combine as many uses. Right. So do you think that should be like a man designing a form of control? Do you know? Do your satellite and your regular tally and your VCR and everything? Well, like maybe what we could use as a sort of like an example of a successful other piece of technology is palm pilots. They're going from being just little scribble boards to cameras, MP3 players, telephones, everything, agenda. So I wonder if we might add something new to the remote control market, such as the lighting in your house. Or even like you need notes about what you want to watch like you might put in there or I want to watch and such and look at that's a good idea and so extra functionalities like personally for me at home I've I've combined the audio video of my television set and my DVD player and my CD player so they all work actually function together but I have different remote controls for each of them. So it's sort of ironic that in their, you know, the sound and everything, it's just one system, but each one's got its own little part. Okay. I'm gonna have to wrap up pretty quickly in the next couple of minutes. I'll just check with nothing else. Okay, so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all? You keep losing them. You keep losing them, okay. You get these ones where you can if you like whistle or make really high pitched noise they beep. I mean is that something we'd want to include do you think? I don don't know. Sure. OK, maybe. I remember when the first remote control my family had was on a cable. Actually, the cable between TV and big buttons that sort of like on a blender or something. My goodness. And you know, and I think about what they are now is better, but actually it's still kind of, I don't know, like a massive junky thing on the table. Maybe we could think about how could be more, you know, streamline. Maybe like touch screen or something. Something like that or whatever would be technologically reasonable. It could be that functionally that doesn't make it any better, but that just the appeal of not having, you know, these days, days every Putes things in people's homes are becoming more and more like chic, you know nicer materials and okay might be Okay, be worth exploring right well, um, so it is to wrap up the next meeting is gonna be in 30 minutes, so that's about um about 10 to 12 by my watch. So in between nine then, as the industrial designer, you're going to be working on, you know, that's a working design of it, so you know what you're doing there. For our user interface, technical functions, I guess that's, you know, like what we've been talking about, what it'll actually do. Marketing executive, you'll be just thinking about what it actually, what requirements it has to fulfill. And you'll get instructions emailed to you, I guess. Okay. Okay. Yes, so it's the functional design stage is next, I guess. And that's the end of the meeting. I got that little message. I locked the scene as an eye thought I would. Before we we wrap up just to make sure we're on the same page here. We're given sort of an example of a coffee machine or something. Are we right now on the assumption that our television remote control may have features which go beyond the television? Or are we keeping sort of like a design commitment to television features? Okay, well just very quickly because it's supposed to finish. No, I guess that's up to us. I mean you probably want some kind of unique selling point of it. So, you know. The factor would be production cost. Yeah. It depends on how much you can cram into that price. Yeah. Okay. Rashed, okay. Well, that's the end of the meeting then. Thank you all for coming. Could I hope that was what they wanted us to do, isn't it? Right? Great. How do you turn this thing off? Is it function and... Oh, there we go. I think if you can just leave it on maybe and then... Oh god, it is. This is all very high-tech, I must say. I don't have a chapter. I'm talking about the SDP. Computer design, all-terrain systems, and the idea. Next semester? Oh, next semester. Computer security. Computer architecture. I don't know. Sorry. I don't know. I don't know. I mean, I don't expecting the five minutes\"}},\n",
       "  {'input_file': 'gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav',\n",
       "   'output_format': 'file',\n",
       "   'output': 'gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav.txt'}]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json_instances = json.dumps(instances)\n",
    "json_instances\n",
    "\n",
    "predict_response = local_endpoint.predict(\n",
    "    request=json_instances,\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "response = json.loads(predict_response.text)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7b611-bc50-40db-8a7b-c3ee3722642f",
   "metadata": {},
   "source": [
    "Great - now that we've tested out local endpoint, lets push it up to the aritifact registry to prepare for deployment in Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "566808ac-48da-4f73-b00f-cf941dbc0f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py-3.9_tf-2.11_aip-1.21/lib/python3.9/subprocess.py:935: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/conda/envs/py-3.9_tf-2.11_aip-1.21/lib/python3.9/subprocess.py:941: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:Using default tag: latest\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:The push refers to repository [us-central1-docker.pkg.dev/gcp-ml-sandbox/vertex-custom-containers/whisper-base-asr-pt-training-gpu]\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:64acb45d4f49: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:d39968f49404: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:08183ab97c71: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:1f11823ff78c: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:2f9392706e5a: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:1d1bf9a3cb96: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:aa7652a10f81: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:45bbe3d22998: Preparing\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:1d1bf9a3cb96: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:aa7652a10f81: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:45bbe3d22998: Waiting\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:1f11823ff78c: Pushed\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:2f9392706e5a: Pushed\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:d39968f49404: Pushed\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:aa7652a10f81: Pushed\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:45bbe3d22998: Pushed\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:08183ab97c71: Pushed\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:64acb45d4f49: Pushed\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:1d1bf9a3cb96: Pushed\n",
      "\n",
      "INFO:google.cloud.aiplatform.docker_utils.local_util:latest: digest: sha256:c61928f293bf944d20e0fc2d63ca61c4f2a5b9e7ddf123b3992d2678aaf6ce56 size: 2004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b098335-c9bb-4727-a5d0-263d8207894e",
   "metadata": {},
   "source": [
    "With our CPR image in the cloud repository, lets know register the local model in the Vertex Registry, adding some tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f159ec6-8b07-477e-9db1-0aabef16aec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model backing LRO: projects/357746845324/locations/us-central1/models/3068906277913493504/operations/7426688467049906176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/357746845324/locations/us-central1/models/3068906277913493504/operations/7426688467049906176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created. Resource name: projects/357746845324/locations/us-central1/models/3068906277913493504@1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/357746845324/locations/us-central1/models/3068906277913493504@1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this Model in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = aiplatform.Model('projects/357746845324/locations/us-central1/models/3068906277913493504@1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/357746845324/locations/us-central1/models/3068906277913493504@1')\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "#PARENT_MODEL='1335653740073451520'\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    local_model=local_model,\n",
    "    #parent_model=PARENT_MODEL,\n",
    "    is_default_version=True,\n",
    "    version_aliases=['hf-pipeline','pytorch'],\n",
    "    labels={'base-image':'pytorch-1-13-1-cuda11-6-cudnn8-runtime'},\n",
    "    display_name='whisper-base-stt-hf-pytorch-gpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a54572-4738-4ffa-a3d9-d183fe105bd9",
   "metadata": {},
   "source": [
    "Now lets create a new Vertex Endpoint and deploy our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "04ea6e44-c401-4fea-8ad8-cfc4591f8ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Endpoint backing LRO: projects/357746845324/locations/us-central1/endpoints/2088346415100067840/operations/8957912340355874816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/357746845324/locations/us-central1/endpoints/2088346415100067840/operations/8957912340355874816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint created. Resource name: projects/357746845324/locations/us-central1/endpoints/2088346415100067840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/357746845324/locations/us-central1/endpoints/2088346415100067840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this Endpoint in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint = aiplatform.Endpoint('projects/357746845324/locations/us-central1/endpoints/2088346415100067840')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/357746845324/locations/us-central1/endpoints/2088346415100067840')\n"
     ]
    }
   ],
   "source": [
    "vertex_endpoint = aiplatform.Endpoint.create(\n",
    "    display_name='whisper-base-stt-endpoint'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708984c9-5ff9-48e7-a03b-c2269982c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name='whisper-base-hf-pipeline',\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type='NVIDIA_TESLA_T4',\n",
    "    traffic_percentage=100,\n",
    "    traffic_split={\"0\":100},\n",
    "    sync=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7923f4-462a-4553-8b55-002cc0bed51b",
   "metadata": {},
   "source": [
    "Lets check the status of our endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "218ebca3-d09b-4391-895a-ed6c3c23a076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: \"2095864875610800128\"\n",
       " model: \"projects/357746845324/locations/us-central1/models/3068906277913493504\"\n",
       " display_name: \"whisper-base-hf-pipeline\"\n",
       " create_time {\n",
       "   seconds: 1679674997\n",
       "   nanos: 429482000\n",
       " }\n",
       " dedicated_resources {\n",
       "   machine_spec {\n",
       "     machine_type: \"n1-standard-4\"\n",
       "     accelerator_type: NVIDIA_TESLA_T4\n",
       "     accelerator_count: 1\n",
       "   }\n",
       "   min_replica_count: 1\n",
       "   max_replica_count: 1\n",
       " }\n",
       " model_version_id: \"1\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "vertex_endpoint = aiplatform.Endpoint('projects/357746845324/locations/us-central1/endpoints/2088346415100067840')\n",
    "\n",
    "vertex_endpoint.list_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48df80f-ded4-4b7b-9a2d-d8519a4cd57e",
   "metadata": {},
   "source": [
    "We can see that our model is deployed now, so lets send it a prediction request for a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1baa29-9681-45a3-a47d-ce4cef904a7d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[{'input_file': 'gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav', 'output_format': 'inline', 'output': {'text': \" Like, gosh, she's already produced a PowerPoint. I think it's already back. Okay. Right? Well, this is the kickoff meeting for our project. And this is just what we're going to be doing over the next 25 minutes. So first of all, just to kind of make sure that we all know each other. I'm Laura and I'm the project manager. Do you want to introduce yourself again? I'm David and I'm supposed to be an industrial designer. Okay. I'm Andrew and I'm a marketing expert. I'm Greg and I'm a user interface. Great. So we're designing a new remote control. Oh, I have to record he's here actually. David, Andrea and Craig? And you all arrive don't time. Yes, we designed a new remote control. As you can see, it's supposed to be original trendy and user friendly. So that's kind of our brief, so we're, and so there are three different stages to the design. I'm not really sure what you guys have already received in your emails. What did you get? I just got the project announcement. So we're going to have like individual work and then a meeting about it and repeat that process three times And at this point you get try out the whiteboard over there So you get to draw your favorite animal and sum up your favorite characteristics of it. So who would like to go first? Very good. All right. So. This one here, right? Okay. Very nice. All right, my favorite basically, high priority for any animal for me is that they be willing to take a lot of physical affection from their family. And yeah, they have lots of personality and be fit in robust good health. So this is Blue Bigo. My family's Bigo.o. Lovely. No, my favourite animal would be a monkey. And then the small, cute and furry and when plant needs becomes real, and we'll be up there with them. You can take as long as this is you like because we haven't got an awful lot to this. What we do, we do. Don't feel like you're in a reaction. I actually told you a whole lot more about Beagles. I don't know what mine is. I'm going to have to think of the spot knife Impressionist Is that a whale? Yeah! Well, I don't know, it's just the first animal I can think of at the top of my head. The figure reason is, because I'm allergic to most animals. I'm allergic to animals first, so fish was a natural toy. And I kind of like wheels and come in and go, everything inside. And if I'm didn't know what I'm gonna write about. Superbscatch, by the way. I was gonna choose a dog as well, but I'll just draw a different kind of dog. My favorite animal is my own dog at home. That doesn't really look like him actually He lets me when I can pick actually I See a dog in there. Yeah, that's very good of you. Now I see a rooster. What kind is it? He's a mixture of various things. And what did I put him? That's just a guess that his tail works. He's very friendly and cheery and always pleased to see you and very kind of affectionate. And he's quite wee as well, so he doesn't take up too much space. And he does a funny thing where he chases his tail as well. This is quite amusing. Is he aware that this is his own tail that he's chasing? It is, I think it is. He only does it after he's had his dinner. And it just all of a sudden his get up and start chasing his tail. And he's around delivering him. Probably when he was little he got lots of attention for doing it and has forever been conditioned. Maybe. Where did you find this just down here? Okay. We'll be doing next. Okay, when I need to discuss the project finance. So according to the brief, we're going to be selling this remote control for 25 euro and we're aiming to make 50 million euro. So we're going to be selling this one in international scale and We don't want it to cost any more than 1250 euros so 50% of the selling price Can we just go over that again sure? So this all right, yeah So cost like production cost is all together 50 but selling prices is that wholesale or retail like on the shelf? I don't know I imagine that's a good question I imagine it probably is or sale actually because it's probably up to the retailer to sell it for whatever price they want. But I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all? Yes. I think it will. Well right away I'm wondering if there's like with DVD players if there are zones. Oh yeah, regions and stuff. Frequency or something. Yeah. Okay. As well as characters, different keypad styles and symbols. Yeah, but for a remote control, do you think that would be as business depends on how complicated our remote control is? Yeah, yeah, yeah, okay, and then and then the other thing international on the topic of price I'm thinking The price might might appeal to a certain market in one region whereas in another it'll be different So it just I can turn just a care to the country like how much money people have to spend on things like basic product positioning the 25 euro remote control might be a big hit in London might not be such a big hit in Greece Good marketing thoughts. Oh gosh, I should be writing all this time. Right away I'm making some kind of assumptions about what information we're given here. Thinking, okay, Trendy probably means something other than just basic, something other than just standard. So I'm wondering right away selling 25 years is that sort of the, is this going to be like the premium product? Yeah, yeah, like how much does, you know, a remote control cost? Well 25 euro, I mean, that's, that's about like 18 pounds or something, isn't it? Or is it as much as that 16 17 18 pounds? I don't know I've never bought a remote control so I don't know how I get a remote control that would get you But yeah, I suppose it has to look kind of cool gimmicky. Okay, let me just scoot on the head here. Okay, does anybody have anything to add to the finance issue at all? Do we have any other background information on how that that compares to other? No actually that would be useful though wouldn't it if you knew like what your money would get you? No. Here interesting thing about discussing production of remote control for me is that as you point out I just don't think remote control is being something something people consciously assess in their purchasing habits. It's just like Getting shoelaces with shoes or something just five minutes to end of meeting. Oh, okay. We're a bit behind. You know what I mean like? Yeah, so sort of like how do you I mean one one way look at it would be well the people producing Television sets maybe they buy remote controls. Or in other ways, maybe people who have TV sets are really fed up with their remote control. And they really want a better one or something. I'm not here in 2021, but I'm more controlled because they got fed up by having four or five different remote controls for each time. Right. Right. And it was just how many devices is a control? Right. So in function, one of the priorities might be to combine as many uses. Right. So do you think that should be like a man designing a form of control? Do you know? Do your satellite and your regular tally and your VCR and everything? Well, like maybe what we could use as a sort of like an example of a successful other piece of technology is palm pilots. They're going from being just little scribble boards to cameras, MP3 players, telephones, everything, agenda. So I wonder if we might add something new to the remote control market, such as the lighting in your house. Or even like you need notes about what you want to watch like you might put in there or I want to watch and such and look at that's a good idea and so extra functionalities like personally for me at home I've I've combined the audio video of my television set and my DVD player and my CD player so they all work actually function together but I have different remote controls for each of them. So it's sort of ironic that in their, you know, the sound and everything, it's just one system, but each one's got its own little part. Okay. I'm gonna have to wrap up pretty quickly in the next couple of minutes. I'll just check with nothing else. Okay, so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all? You keep losing them. You keep losing them, okay. You get these ones where you can if you like whistle or make really high pitched noise they beep. I mean is that something we'd want to include do you think? I don don't know. Sure. OK, maybe. I remember when the first remote control my family had was on a cable. Actually, the cable between TV and big buttons that sort of like on a blender or something. My goodness. And you know, and I think about what they are now is better, but actually it's still kind of, I don't know, like a massive junky thing on the table. Maybe we could think about how could be more, you know, streamline. Maybe like touch screen or something. Something like that or whatever would be technologically reasonable. It could be that functionally that doesn't make it any better, but that just the appeal of not having, you know, these days, days every Putes things in people's homes are becoming more and more like chic, you know nicer materials and okay might be Okay, be worth exploring right well, um, so it is to wrap up the next meeting is gonna be in 30 minutes, so that's about um about 10 to 12 by my watch. So in between nine then, as the industrial designer, you're going to be working on, you know, that's a working design of it, so you know what you're doing there. For our user interface, technical functions, I guess that's, you know, like what we've been talking about, what it'll actually do. Marketing executive, you'll be just thinking about what it actually, what requirements it has to fulfill. And you'll get instructions emailed to you, I guess. Okay. Okay. Yes, so it's the functional design stage is next, I guess. And that's the end of the meeting. I got that little message. I locked the scene as an eye thought I would. Before we we wrap up just to make sure we're on the same page here. We're given sort of an example of a coffee machine or something. Are we right now on the assumption that our television remote control may have features which go beyond the television? Or are we keeping sort of like a design commitment to television features? Okay, well just very quickly because it's supposed to finish. No, I guess that's up to us. I mean you probably want some kind of unique selling point of it. So, you know. The factor would be production cost. Yeah. It depends on how much you can cram into that price. Yeah. Okay. Rashed, okay. Well, that's the end of the meeting then. Thank you all for coming. Could I hope that was what they wanted us to do, isn't it? Right? Great. How do you turn this thing off? Is it function and... Oh, there we go. I think if you can just leave it on maybe and then... Oh god, it is. This is all very high-tech, I must say. I don't have a chapter. I'm talking about the SDP. Computer design, all-terrain systems, and the idea. Next semester? Oh, next semester. Computer security. Computer architecture. I don't know. Sorry. I don't know. I don't know. I mean, I don't expecting the five minutes\"}}], deployed_model_id='2095864875610800128', model_version_id='1', model_resource_name='projects/357746845324/locations/us-central1/models/3068906277913493504', explanations=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "instances = [\n",
    "                {\"input_file\":\"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav\",\"output_format\":\"inline\"},\n",
    "            ]\n",
    "\n",
    "prediction = vertex_endpoint.predict(instances)\n",
    "prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63bec9-613f-41ef-9831-8c174944275c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Great - we were able to get a realtime transcription of a 20 min .wav file in less than the 60s timeout of the endpoint.  Pretty amazing!\n",
    "\n",
    "### Batch Prediction\n",
    "Now lets prepare some data for Batch Prediction where we will transcribe a set of audio files and write the results back out to cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc931f14-4140-47ed-84c1-bc44c5db9bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio/amicorpus'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Root audio directory:\n",
    "BUCKET=\"gcp-ml-sandbox-whisper\"\n",
    "AUDIO_ROOT=\"audio/amicorpus\"\n",
    "AUDIO_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295b285-96fb-4e43-838a-287c7f5cc955",
   "metadata": {},
   "source": [
    "Let's list out the audio files in cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495d1ff7-f8c3-49e0-a1a7-fe29c6f4c8a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002c/audio/ES2002c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002d/audio/ES2002d.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003a/audio/ES2003a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003b/audio/ES2003b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003c/audio/ES2003c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003d/audio/ES2003d.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004a/audio/ES2004a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004b/audio/ES2004b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004c/audio/ES2004c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004d/audio/ES2004d.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005a/audio/ES2005a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005b/audio/ES2005b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005c/audio/ES2005c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005d/audio/ES2005d.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006a/audio/ES2006a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006b/audio/ES2006b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006c/audio/ES2006c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006d/audio/ES2006d.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007a/audio/ES2007a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007b/audio/ES2007b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007c/audio/ES2007c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007d/audio/ES2007d.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008a/audio/ES2008a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008b/audio/ES2008b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008c/audio/ES2008c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008d/audio/ES2008d.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009a/audio/ES2009a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009b/audio/ES2009b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009c/audio/ES2009c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009d/audio/ES2009d.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010a/audio/ES2010a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010b/audio/ES2010b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010c/audio/ES2010c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010d/audio/ES2010d.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011a/audio/ES2011a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011b/audio/ES2011b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011c/audio/ES2011c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011d/audio/ES2011d.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012a/audio/ES2012a.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012b/audio/ES2012b.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012c/audio/ES2012c.Mix-Headset.wav\n",
      "gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012d/audio/ES2012d.Mix-Headset.wav\n"
     ]
    }
   ],
   "source": [
    "AUDIO_GLOB_PATH=f\"gs://{BUCKET}/{AUDIO_ROOT}/**/*.wav\"\n",
    "!gsutil ls $AUDIO_GLOB_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4930fe98-bca6-424e-8827-ca8669b673b9",
   "metadata": {},
   "source": [
    "Now, lets create an output file that contains a listing of our audio files and copy it up to cloud storage for Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e2b826-93e5-41d6-af7f-e381f5b5fce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_files=!gsutil ls  $AUDIO_GLOB_PATH\n",
    "with open('amicorpus.jsonl', 'w') as w:\n",
    "    for file in audio_files:\n",
    "        #w.write(f'\\{\"input_file\":\"{file}\",\"output_format\":\"file\"\\}\\n')\n",
    "        w.write(f'{{\"input_file\":\"{file}\",\"output_format\":\"file\"}}\\n')\n",
    "    w.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58000edc-926c-4fc9-a114-2b104c211de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://amicorpus.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  5.2 KiB/  5.2 KiB]                                                \n",
      "Operation completed over 1 objects/5.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "TARGET_DIR=f\"gs://{BUCKET}/{AUDIO_ROOT}\"\n",
    "!gsutil cp amicorpus.jsonl $TARGET_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528df1e6-7821-4948-8a30-c2239769a48b",
   "metadata": {},
   "source": [
    "With our files in place, let's kickoff our batch prediction job in vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e56f5f91-cce8-4f8c-a18a-9c9a8afb805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n",
      "BatchPredictionJob created. Resource name: projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720\n",
      "To use this BatchPredictionJob in another session:\n",
      "bpj = aiplatform.BatchPredictionJob('projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/8118503176111390720?project=357746845324\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Grabbing a reference to our model from our earlier execution:\n",
    "model = aiplatform.Model('projects/357746845324/locations/us-central1/models/3068906277913493504@1')\n",
    "\n",
    "bp_job = aiplatform.BatchPredictionJob.create(\n",
    "    job_display_name='whisper-base-asr-stt',\n",
    "    model_name=model,\n",
    "    instances_format='jsonl',\n",
    "    predictions_format= 'jsonl',\n",
    "    gcs_source=[f'{TARGET_DIR}/amicorpus.jsonl'],\n",
    "    gcs_destination_prefix=f'{TARGET_DIR}/batch-prediction-output',\n",
    "    machine_type='n1-standard-4',\n",
    "    accelerator_type='NVIDIA_TESLA_T4',\n",
    "    accelerator_count=1,\n",
    "    starting_replica_count= 2,\n",
    "    max_replica_count=2,\n",
    "    sync=False,\n",
    "    batch_size=2,\n",
    "    service_account='357746845324-compute@developer.gserviceaccount.com'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ba42f-aab6-4720-bca4-1e16478b9336",
   "metadata": {},
   "source": [
    "Lets check the state of the batch job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbfd9770-c74f-464c-aad5-17181f3ad749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/357746845324/locations/us-central1/batchPredictionJobs/8118503176111390720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "bp_job.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "979c98ec-e14a-4811-8c38-a11b2e979fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_job.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfcc5bd-57e1-4ed7-a858-9f8bee8a0c1f",
   "metadata": {},
   "source": [
    "We can iterate over the batch job to look at the results which gives us a `google.cloud.storage.blob.Blob` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f60acb10-0f2e-454c-b4cb-605e52571580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003b/audio/ES2003b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003b/audio/ES2003b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003b/audio/ES2003b.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003c/audio/ES2003c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003c/audio/ES2003c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003c/audio/ES2003c.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003d/audio/ES2003d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003d/audio/ES2003d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003d/audio/ES2003d.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007a/audio/ES2007a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007a/audio/ES2007a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007a/audio/ES2007a.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007b/audio/ES2007b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007b/audio/ES2007b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007b/audio/ES2007b.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007c/audio/ES2007c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007c/audio/ES2007c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007c/audio/ES2007c.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002a/audio/ES2002a.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002b/audio/ES2002b.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002c/audio/ES2002c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002c/audio/ES2002c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002c/audio/ES2002c.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002d/audio/ES2002d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002d/audio/ES2002d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2002d/audio/ES2002d.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003a/audio/ES2003a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003a/audio/ES2003a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2003a/audio/ES2003a.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009c/audio/ES2009c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009c/audio/ES2009c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009c/audio/ES2009c.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009d/audio/ES2009d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009d/audio/ES2009d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009d/audio/ES2009d.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010a/audio/ES2010a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010a/audio/ES2010a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010a/audio/ES2010a.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010b/audio/ES2010b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010b/audio/ES2010b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010b/audio/ES2010b.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010c/audio/ES2010c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010c/audio/ES2010c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010c/audio/ES2010c.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004a/audio/ES2004a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004a/audio/ES2004a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004a/audio/ES2004a.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004b/audio/ES2004b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004b/audio/ES2004b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004b/audio/ES2004b.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004c/audio/ES2004c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004c/audio/ES2004c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004c/audio/ES2004c.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004d/audio/ES2004d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004d/audio/ES2004d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2004d/audio/ES2004d.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005a/audio/ES2005a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005a/audio/ES2005a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005a/audio/ES2005a.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010d/audio/ES2010d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010d/audio/ES2010d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2010d/audio/ES2010d.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011a/audio/ES2011a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011a/audio/ES2011a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011a/audio/ES2011a.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011b/audio/ES2011b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011b/audio/ES2011b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011b/audio/ES2011b.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011c/audio/ES2011c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011c/audio/ES2011c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011c/audio/ES2011c.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012c/audio/ES2012c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012c/audio/ES2012c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012c/audio/ES2012c.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011d/audio/ES2011d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011d/audio/ES2011d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2011d/audio/ES2011d.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012a/audio/ES2012a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012a/audio/ES2012a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012a/audio/ES2012a.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012b/audio/ES2012b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012b/audio/ES2012b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012b/audio/ES2012b.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005b/audio/ES2005b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005b/audio/ES2005b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005b/audio/ES2005b.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005c/audio/ES2005c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005c/audio/ES2005c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005c/audio/ES2005c.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005d/audio/ES2005d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005d/audio/ES2005d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2005d/audio/ES2005d.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006a/audio/ES2006a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006a/audio/ES2006a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006a/audio/ES2006a.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006b/audio/ES2006b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006b/audio/ES2006b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006b/audio/ES2006b.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006c/audio/ES2006c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006c/audio/ES2006c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006c/audio/ES2006c.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006d/audio/ES2006d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006d/audio/ES2006d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2006d/audio/ES2006d.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007d/audio/ES2007d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007d/audio/ES2007d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2007d/audio/ES2007d.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008a/audio/ES2008a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008a/audio/ES2008a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008a/audio/ES2008a.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008b/audio/ES2008b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008b/audio/ES2008b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008b/audio/ES2008b.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008c/audio/ES2008c.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008c/audio/ES2008c.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008c/audio/ES2008c.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008d/audio/ES2008d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008d/audio/ES2008d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2008d/audio/ES2008d.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009a/audio/ES2009a.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009a/audio/ES2009a.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009a/audio/ES2009a.Mix-Headset.wav.txt\"}}\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009b/audio/ES2009b.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009b/audio/ES2009b.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2009b/audio/ES2009b.Mix-Headset.wav.txt\"}}\n",
      "\n",
      "{\"instance\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012d/audio/ES2012d.Mix-Headset.wav\", \"output_format\": \"file\"}, \"prediction\": {\"input_file\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012d/audio/ES2012d.Mix-Headset.wav\", \"output_format\": \"file\", \"output\": \"gs://gcp-ml-sandbox-whisper/audio/amicorpus/ES2012d/audio/ES2012d.Mix-Headset.wav.txt\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "prediction_output = []\n",
    "for output_file in bp_job.iter_outputs():\n",
    "    print(output_file.download_as_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a44b92-c07d-4ee4-87eb-ddaa459fe030",
   "metadata": {},
   "source": [
    "## Ideas for future testing\n",
    " * Finding the optimal # of workers for a given accelerator to best maximize GPU memory\n",
    " * Testing out with multiple accelerators on an instance (e.g 4x T4s)\n",
    " * Using BetterTransformers in the pipeline to improve inference performance\n",
    " * Testing different batch sizes in pipeline code\n",
    " * Testing different batch sizes in BatchPrediction code\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25167520-f2ea-4143-ae92-45b4addaf779",
   "metadata": {},
   "source": [
    "## Graveyard\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2a996-710f-4441-a950-a0bc752bb10b",
   "metadata": {},
   "source": [
    "### DEBUGGING\n",
    "Running CPR container manually with `bash` entrypoint on port 8081\n",
    "```\n",
    "docker run -it --entrypoint bash --gpus all \\\n",
    "--env VERTEX_CPR_MAX_WORKERS=1 \\\n",
    "--env AIP_HTTP_PORT=8080 --env AIP_HEALTH_ROUTE='/' \\\n",
    "--env AIP_PREDICT_ROUTE='/predict' \\\n",
    "-p 0.0.0.0:8081:8080/tcp us-central1-docker.pkg.dev/gcp-ml-sandbox/vertex-custom-containers/whisper-base-asr-pt-training-gpu\n",
    "```\n",
    "\n",
    "Running CPR in container\n",
    "\n",
    "```\n",
    "python -m google.cloud.aiplatform.prediction.model_server\n",
    "```\n",
    "\n",
    "Installing missing libsndfile library in container\n",
    "\n",
    "```\n",
    "subprocess.run([\"apt update && apt install libsndfile1 -y\"], shell=True,capture_output=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b64329-b6bc-4e38-a37e-652337ecf7cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiments/Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d1444-97b3-4d56-b1b2-7ec2aea85285",
   "metadata": {},
   "source": [
    "Testing out HuggingFace Whisper TensorFlow model with XLA compilation for inference speedup.  Note, this model is limited to 30 second audio samples and will clip any audio beyond that duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936bdc0-e467-4253-90c9-f8d8c76f0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoProcessor, TFWhisperForConditionalGeneration\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def generate_transcriptions(audio_files: list[dict]):\n",
    "    \n",
    "    def load_model(model_name=\"openai/whisper-base\"):\n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "        model_instance = TFWhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        generator = tf.function(model_instance.generate, jit_compile=True)\n",
    "        \n",
    "        return processor, generator\n",
    "    \n",
    "    def get_input_features(audio_array, sampling_rate, processor):\n",
    "            return processor(audio_array,sampling_rate=sampling_rate, return_tensors=\"tf\").input_features\n",
    "\n",
    "    def get_transcription(input_features,processor, generator):\n",
    "        generated_ids = generator(input_features)\n",
    "        return processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    def upload_transcription(file,contents):\n",
    "        filename = file.split('/')[-1]\n",
    "        gcs_path = file.split(filename)[0]\n",
    "        output_filename = f\"{filename}.txt\"\n",
    "\n",
    "        bucket_name = file.split('/')[2]\n",
    "        destination_blob_name = '/'.join(file.split('/')[3:-1]) + \"/\" + output_filename\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_string(contents)\n",
    "        return gcs_path + output_filename\n",
    "    \n",
    "    transcriptions =[]\n",
    "    \n",
    "    \n",
    "    audio_dataset = Dataset.from_dict({\"audio\": audio_files}).cast_column(\"audio\",Audio(sampling_rate=16000))    \n",
    "    processor, generator = load_model()\n",
    "    \n",
    "    for file in audio_dataset:\n",
    "        input_features = get_input_features(file['audio']['array'], audio_dataset.features[\"audio\"].sampling_rate,processor)\n",
    "        \n",
    "        t=get_transcription(input_features, processor, generator)\n",
    "        output_file = upload_transcription(file['audio']['path'],t)\n",
    "        transcriptions.append(output_file)\n",
    "    \n",
    "    return transcriptions\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f37cac44-ce55-49a3-890e-89ccd0b6fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFWhisperForConditionalGeneration.\n",
      "\n",
      "All the layers of TFWhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWhisperForConditionalGeneration for predictions without further training.\n",
      "2023-02-28 20:26:42.822442: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at scatter_nd_op.cc:216 : INVALID_ARGUMENT: indices[0] = [0, -1] does not index into shape [1,51865]\n",
      "2023-02-28 20:26:42.822997: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at scatter_nd_op.cc:216 : INVALID_ARGUMENT: indices[0] = [0, -1] does not index into shape [1,51865]\n",
      "2023-02-28 20:26:42.823676: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at scatter_nd_op.cc:216 : INVALID_ARGUMENT: indices[0] = [0, -1] does not index into shape [1,51865]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0000.flac': ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0001.flac': \" Nor is Mr. Quilter's manner less interesting than his matter.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0002.flac': ' He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0003.flac': \" He has graved doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of Rocky Ithaca.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0004.flac': \" Linnell's pictures are a sort of upgards and atom paintings, and Mason's exquisite ittles are as national as a jingo poem. Mr. Birkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back before he says like a shampooer in a Turkish bath. Next man\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0005.flac': ' It is obviously unnecessary for us to point out how luminous these criticisms are, how delicate in expression.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0006.flac': ' On the general principles of art, Mr. Krilter writes with equal lucidity.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0007.flac': ' Painting, he tells us, is of a different quality to mathematics, and finish in art is adding more factor.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0008.flac': ' As for etchings, there are two kinds, British and foreign.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0009.flac': ' He laments most bitterly the divorce that has been made between decorative art and what we usually call pictures. Mixed a customary appeal to the last judgment and reminds us that in the great days of art Michelangelo was the furnishing upholsterer.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0010.flac': ' near the fire, and the ornaments Fred brought home from India on the mental board.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0011.flac': ' In fact, he is quite severe on Mr. Ruskin for not recognizing that a picture should denote the frailty of man, and remarks with pleasing courtesy and felicitous grace that many phases of feeling',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0012.flac': ' Only, unfortunately, his own work never does get good.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0013.flac': ' Mr. Quilter has missed his chance, for he has failed even to make himself the Tupper of Painting.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/128104/1272-128104-0014.flac': ' by Harry quilter MA.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0000.flac': ' Because you were sleeping instead of conquering, the lovely rose princess has become a fiddle without a bow. All porous shaggy sits there, accuing dove.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0001.flac': ' He has gone and gone for good, answered polychrome who had managed to squeeze into the room beside the dragon and had witnessed the occurrences with much interest.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0002.flac': ' I have remained a prisoner only because I wished to be one. And with this, he stepped forward and burst the stout chains as easily as if they had been threads.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0003.flac': ' The little girl had been asleep, but she heard the rap and opened the door.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0004.flac': ' The king is flooded disgrace and your friends are asking for you.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0005.flac': ' I begged Ruggedo long ago to send him away, but he would not do so.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0006.flac': ' I also offered to help your brother to escape, but he would not go.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0007.flac': ' He eats and sleeps very steadily, replay the nooking.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0008.flac': \" I hope he doesn't work too hard since Shaggy.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0009.flac': \" He doesn't work at all.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0010.flac': ' In fact, there is nothing he can do in these dominions as well as our norms, whose numbers are so great that it worries us to keep them all busy.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0011.flac': \" Not exactly, we've turned Calico.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0012.flac': ' Where is my brother now?',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0013.flac': ' Inquired Shaggy in the Metal Forest.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0014.flac': ' Where is that?',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0015.flac': ' The middle forest is in the Great Donde Cavern, the largest in all our Dominions, Ripleyed Calico.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0016.flac': ' Callergo hesitated.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0017.flac': ' However, if we look sharp, we may be able to discover one of these secret ways.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0018.flac': \" Oh no, I'm quite sure he didn't.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0019.flac': \" That's funny, remarked the bedside thoughtfully.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0020.flac': \" I don't believe Ann knew any magic or she'd have worked it before.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0021.flac': ' I do not know, confess to Shaggy.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0022.flac': ' True, agreed Calico.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0023.flac': ' Calico went to the big gong and pounded on it just as regular used to do, but no one answered the summons.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/135031/1272-135031-0024.flac': ' Having returned to the Royal Cavern, Calico first pounded the gong and then sat in the throne, wearing regidos discarded ruby crown and holding in his hand the scepter which regidos had so often thrown at his head.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0000.flac': ' A man said to the universe, Sir, I exist.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0001.flac': \" Sweat-covered Brian's body, trickling into the titling cloth that was the only garment he wore.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0002.flac': ' The cut on his chest is still dripping blood. The ache of his overstrained eyes, even the soaring arena around him with the thousands of spectators, retrievalities not worth thinking about.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0003.flac': ' His instant panic was followed by a small sharp blow high on his chest.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0004.flac': ' One minute, a voice said, and the timbers are sounded.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0005.flac': ' A minute is not a very large measure of time, and his body needed every fraction of it.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0006.flac': ' The buzzers were triggered as muscles into complete relaxation.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0007.flac': \" O Lee's heart and lungs worked on at a strong, measured rate.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0008.flac': ' He was in reverie, sliding along the borders of consciousness.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0009.flac': ' The contestants in the 20s needed undisturbed rest. Therefore, knights in the dormitories were as quiet as death.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0010.flac': ' Particularly so, on this last night, when only two of the little cubicles were occupied, the thousands of others standing with dark empty doors.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0011.flac': ' The other voice snapped with a harsh urgency clearly used to command.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0012.flac': \" I'm here because the matter is of utmost importance, and brand is the one I must see. Now stand aside.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0013.flac': ' The 20s.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0014.flac': \" He must have drawn his gun because the intruder said quickly, but that away you're being a fool. Out.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0015.flac': ' There is silence then, and still wondering, Brienne was once more asleep.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0016.flac': ' 10 seconds.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0017.flac': ' He asked the handler who was needing his aching muscles.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0018.flac': ' A red-haired mountain of a man was an apparently inexhaustible store of energy.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0019.flac': ' There could be a little art in this last and final round of fencing.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0020.flac': ' Just thrust and parry and victory to the stronger.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0021.flac': ' Every man who entered the 20s had his own training tricks.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0022.flac': ' There appeared to be an immediate association with the death trauma, as if the two were inextricably linked into one.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0023.flac': ' The strength that enables someone in a trance to hold his body stiff and unsupported, except at two points, the head and heels.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0024.flac': ' This is physically impossible when conscious.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0025.flac': ' Others had died before during the 20s, and death during the last round was in some ways easier than defeat.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0026.flac': \" Breathing deeply, Briann's softly spoke the auto hypnotic phrases that triggered the process.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0027.flac': ' When the buzzer sounded, he pulled his foil from his second startled grasp and ran forward.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0028.flac': ' I rolled click the maze at the sudden fury of the attack, then smiled.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0029.flac': ' He said it was the last burst of energy. He knew how close they both were to exhaustion.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0030.flac': \" Brianna saw something close to panic on his opponent's face when the man finally recognized his error.\",\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0031.flac': ' A wave of despair rolled out from Irolgg. Brian sensed it and knew the fifth point was his.',\n",
       " 'gs://gcp-ml-sandbox-whisper/audio/librispeech_asr_dummy/1272/141231/1272-141231-0032.flac': \" Then the powerful twist that's rest of the side, in and under the guard.\"}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = generate_transcriptions(audio_files)\n",
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d870ae1-6973-4761-acab-0d8c984d55a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = xla_generate(input_features)\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df146af6-c78c-4902-a5c1-4127c635b984",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "py-3.9_tf-2.11_aip-1.21",
   "name": "common-cpu.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m93"
  },
  "kernelspec": {
   "display_name": "py-3.9_tf-2.11_aip-1.21",
   "language": "python",
   "name": "py-3.9_tf-2.11_aip-1.21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
