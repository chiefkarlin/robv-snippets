{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we demonstrate creating an ephermal Ray cluster to run a workload using the ray [RLlib](https://docs.ray.io/en/latest/rllib/index.html) reinforcment learning library on top of Vertex AI training.  \n",
    "\n",
    "### Creating an ephemeral Ray cluster using Vertex AI Training\n",
    "\n",
    "The following notebook demonstrates a basic proof-of-concept example for creating a Ray cluster on demand for reinforcement learning using the framework-agnostic distributed training capabilities of Vertex AI.\n",
    "\n",
    "At a high level, we create a single custom container for use by both the 'head' node and the 'worker' nodes in the Ray cluster. At the start of training, each node checks for its location in the resource pool by inspecting the Vertex AI 'task' attribute of the [`CLUSTER_SPEC`](https://cloud.google.com/vertex-ai/docs/training/distributed-training#cluster-variables) environment variable - the first node in `workerpool0` becomes the 'head' node and nodes in `workerpool1` become workers which contribute compute resources to the cluster.\n",
    "\n",
    "Ray doesn't have a simple way of shutting down the whole cluster and cleanly releasing all the resources, so upon completion of the training script, the head node creates a `stop_cluster.txt` file in the gcs storage bucket before issuing `ray stop` which should send terminate commands to the workers.  The head node will wait for one minute before issuing `ray stop` which should give the workers some time to check for the existence of the `stop_cluster.txt` and shut themselves down cleanly. Some connection errors may be reported in Cloud logging as the worker nodes are shutting down and no longer able to connect to the head node.\n",
    "\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI Training\n",
    "* Vertex AI Experiments (Managed Tensorboard)\n",
    "* Artifact Registry\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Artifact Registry pricing](https://cloud.google.com/artifact-registry/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install additional package dependencies not installed in your notebook environment, such as the Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyy5Lbnzg5fi",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  gcp-ml-sandbox\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20221206201057'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "\n",
    "Ray has [built-in support for Tensorboard](https://docs.ray.io/en/latest/tune/api_docs/logging.html).  We will create a Cloud Storage bucket to store logging output of our reinforcement learning job for access by Vertex managed Tensorboard.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. We suggest that you [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"-aip-\" + TIMESTAMP\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://gcp-ml-sandbox-aip-20221206201057/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Python and Env Variables for building containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a series of variables that will assist us building our custom container and specifying the resources we need to execute our custom job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_NAME='ray-ml'\n",
    "\n",
    "NODE_DIR=\"/home/jupyter/ray/node\"\n",
    "# Adjust to reflect package requirements\n",
    "NODE_DIR=\"/home/jupyter/ray/trainer\"\n",
    "SETUP_DIR=\"/home/jupyter/ray\"\n",
    "TRAINER_DIR=\"/ray/trainer\"\n",
    "\n",
    "NODE_IMAGE_NAME='ray_cluster_node'\n",
    "NODE_IMAGE_TAG='latest'\n",
    "NODE_IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{NODE_IMAGE_NAME}:{NODE_IMAGE_TAG}\"\n",
    "\n",
    "API_ENDPOINT=f\"{REGION}-aiplatform.googleapis.com\"\n",
    "\n",
    "    \n",
    "os.environ['PROJECT_ID']=PROJECT_ID\n",
    "os.environ['NODE_DIR']=NODE_DIR\n",
    "os.environ['NODE_IMAGE_URI']=NODE_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Working directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a working directory for writing our training job and Dockerfile definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/jupyter/ray/trainer created successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs(os.getenv(\"HOME\") + TRAINER_DIR, exist_ok = True)\n",
    "    print(f\"Directory {os.getenv('HOME') + TRAINER_DIR} created successfully\")\n",
    "except OSError as error:\n",
    "    print (error)\n",
    "    print(\"Directory '%s' exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/ray/trainer\n"
     ]
    }
   ],
   "source": [
    "%cd $NODE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/jupyter/ray/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SETUP_DIR}/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"ray[rllib]\",\n",
    "    \"gym\",\n",
    "    #\"torch\",\n",
    "    \"tblib\"\n",
    "]\n",
    "\n",
    "VERSION = '0.1'\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version=VERSION,\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='My training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('__init__.py', 'w') as fp:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following training script implements an example of the [IMPALA RL algorithm](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#impala) from the Ray RLLib library.  \n",
    "\n",
    "The code specifies to use GPU (cuda) devices for the environment configuration.  The environment setup here should match the resources being requested as part of the Vertex job being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/jupyter/ray/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {NODE_DIR}/task.py\n",
    "from collections import Counter\n",
    "import socket\n",
    "import time\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from gym import spaces, logger\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "# get random password pf length 8 with letters, digits\n",
    "characters = string.ascii_letters + string.digits\n",
    "ray_pwd = ''.join(random.choice(characters) for i in range(8))\n",
    "\n",
    "# Stop cluster file for cleanly shutting down workers\n",
    "model_dir = os.getenv(\"AIP_MODEL_DIR\", 'trainer')\n",
    "print(f\"AIP_MODEL_DIR: {model_dir}\")\n",
    "\n",
    "start_file_name = 'cluster_started.txt'\n",
    "stop_file_name = 'stop_cluster.txt'\n",
    "\n",
    "#bucket = model_dir.split(\"/\")[2]\n",
    "bucket = \"vertexai-mlops-workshop\"\n",
    "#parent = (\"/\").join(model_dir.split(\"/\")[3:])\n",
    "parent = \"ray-rllib-custom-job\"\n",
    "stop_name = parent + stop_file_name\n",
    "start_name = parent + start_file_name\n",
    "\n",
    "stop_file_name = f\"/gcs/{bucket}/{stop_name}\"\n",
    "start_file_name = f\"/gcs/{bucket}/{start_name}\"\n",
    "\n",
    "print(f\"Stop file name: {stop_file_name}\")\n",
    "print(f\"Cluster Started file name: {start_file_name}\")\n",
    "\n",
    "\n",
    "#Cluster info\n",
    "cluster_spec = json.loads(os.environ.get(\"CLUSTER_SPEC\"))\n",
    "print(f\"ClusterSpec:\\n{cluster_spec}\")\n",
    "\n",
    "head_node = cluster_spec['cluster']['workerpool0'][0].split(\":\")[0]\n",
    "head_node_address = f'{head_node}:2222'\n",
    "\n",
    "#Grab the task variable to see which which node pool we are in.  \n",
    "#If workerpool0, we are the head node, otherwise, we are a worker node to delegate resources to the cluster\n",
    "\n",
    "head_node = (cluster_spec['task']['type'] == 'workerpool0')\n",
    "\n",
    "if head_node: \n",
    "    print(f\"Running as Head node: {head_node}\")\n",
    "    print(f\"Head node_address: {head_node_address}\")\n",
    "    os.system(f\"ray start --head --port=2222 --redis-password={ray_pwd}\")\n",
    "    os.system(f\"mkdir -p /gcs/{bucket}/{parent}\")\n",
    "    os.system(f\"touch {start_file_name}\")\n",
    "\n",
    "else: # worker - connect and sleep\n",
    "    os.system(f\"ray start --address={head_node_address} --redis-password={ray_pwd}\")\n",
    "\n",
    "ray.init(address=\"auto\",_redis_password=ray_pwd)\n",
    "\n",
    "\n",
    "#WORK\n",
    "class CartPole(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 50\n",
    "    }\n",
    "\n",
    "    def __init__(self, config: EnvContext):\n",
    "        #print(config)\n",
    "        #print(\"CUDA Here?: \", torch.cuda.is_available())\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "        self.env_count = config[\"env_count\"]\n",
    "        self.device=config[\"device\"]\n",
    "        \n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "        self.done = torch.full([self.env_count], True, dtype=torch.bool, device=self.device)\n",
    "        self.state = torch.zeros([self.env_count, 4], dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #breakpoint()\n",
    "        # All env must already have been reset.\n",
    "        self.done[:] = False\n",
    "        x, x_dot, theta, theta_dot = self.state[:, 0], self.state[:, 1], self.state[:, 2], self.state[:, 3]\n",
    "        #breakpoint()\n",
    "        force = self.force_mag * ((action * 2.) - 1.)\n",
    "        \n",
    "        costheta = torch.cos(theta)\n",
    "        sintheta = torch.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = ((self.gravity * sintheta - costheta * temp) \n",
    "                    / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state[:, 0], self.state[:, 1], self.state[:, 2], self.state[:, 3] = x, x_dot, theta, theta_dot\n",
    "\n",
    "        self.done = (\n",
    "            (x < -self.x_threshold)\n",
    "            | (x > self.x_threshold)\n",
    "            | (theta < -self.theta_threshold_radians)\n",
    "            | (theta > self.theta_threshold_radians)\n",
    "        )\n",
    "        reward = ~self.done\n",
    "        \n",
    "        #self.state = self.reset()\n",
    "        self.reset()\n",
    "        #print(\"In GPU: \", self.state.is_cuda)\n",
    "        to_return = self.state.to(\"cpu\").numpy()[0]\n",
    "        return to_return, reward.float().item(), self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        #breakpoint()\n",
    "        self.state = torch.where(self.done.unsqueeze(1), (torch.rand(self.env_count, 4, device=self.device) -0.5) / 10., self.state) \n",
    "        #self.state = (torch.rand((self.env_count, 4)) -0.5) / 10.\n",
    "        #print(\"Before converting to CPU, self.state=\", self.state)\n",
    "        #print(self.state)\n",
    "        to_return = self.state.to(\"cpu\").numpy()[0]\n",
    "        #print(\"After converting to CPU, to_return= \", to_return)\n",
    "        return to_return\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(.8, .6, .4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5, .5, .8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "if head_node:\n",
    "    \n",
    "    print('''This cluster consists of\n",
    "    {} nodes in total\n",
    "    {} CPU resources in total\n",
    "    {} GPU resources in total\n",
    "    '''.format(len(ray.nodes()), ray.cluster_resources()['CPU'],ray.cluster_resources()['GPU']))\n",
    "    \n",
    "    #tensorboard_dir = os.environ.get(\"AIP_TENSORBOARD_LOG_DIR\")\n",
    "    tensorboard_dir = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\", 'tb_dir')\n",
    "    #tb_bucket = tensorboard_dir.split(\"/\")[2]\n",
    "    #tb_path = (\"/\").join(tensorboard_dir.split(\"/\")[3:])\n",
    "    tb_path = f\"{parent}/{tensorboard_dir}\"\n",
    "\n",
    "    #ray_log_path = f\"/gcs/{tb_bucket}/{tb_path}\"\n",
    "    ray_log_path = f\"/gcs/{bucket}/{tb_path}\"\n",
    "\n",
    "    print(f\"Ray Output Path: {ray_log_path}\")\n",
    "    \n",
    "    if not os.path.exists(ray_log_path):\n",
    "        os.makedirs(ray_log_path)\n",
    "\n",
    "    #RL WORK\n",
    "    register_env(\"CartPole\", lambda config: CartPole(config))\n",
    "    \n",
    "    #ENV config\n",
    "    rollout_fragment_length = 128\n",
    "    config = dict(\n",
    "            {\n",
    "            \"num_workers\": 1,\n",
    "            \"num_aggregation_workers\": 0,\n",
    "            \"num_cpus_for_driver\": 2,\n",
    "            \"num_gpus\": 1,\n",
    "            \"num_gpus_per_worker\": 0.5,\n",
    "            \"num_cpus_per_worker\": 4,\n",
    "            \"num_envs_per_worker\": 1,\n",
    "            \"num_multi_gpu_tower_stacks\": 1,\n",
    "            \"minibatch_buffer_size\": 1,\n",
    "            \"num_sgd_iter\" : 1,\n",
    "            \"vf_loss_coeff\": 0.5,\n",
    "            \"train_batch_size\": rollout_fragment_length*4,\n",
    "            },\n",
    "            **{\n",
    "                \"env\": CartPole,\n",
    "                \"env_config\": {\n",
    "                    \"env_count\": 1,\n",
    "                    \"device\": \"cuda\",\n",
    "                },\n",
    "                \"model\": {\n",
    "                    \"fcnet_hiddens\": [256, 256],\n",
    "                    \"use_lstm\": True,\n",
    "                    \"lstm_cell_size\": 256,\n",
    "                    \"lstm_use_prev_action\": False,\n",
    "                    \"lstm_use_prev_reward\": False,\n",
    "                },\n",
    "                \"framework\": \"torch\",\n",
    "                # Run with tracing enabled for tfe/tf2?\n",
    "                \"eager_tracing\": False,\n",
    "            })\n",
    "    \n",
    "    stop = {\n",
    "        \"training_iteration\": 100\n",
    "    }\n",
    "\n",
    "    results = tune.run(\"IMPALA\", config=config, stop=stop, verbose=2, local_dir=ray_log_path, reuse_actors=True)\n",
    "\n",
    "\n",
    "    #Shutdown the cluster\n",
    "    print ('Shutting down cluster')\n",
    "    open(stop_file_name, 'w').close()\n",
    "    time.sleep(60) # Give workers a chance to detect the stop_cluster.txt file and cleanly shutdown\n",
    "    os.system(\"ray stop\")\n",
    "    os.remove(stop_file_name)\n",
    "    os.remove(start_file_name)\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"Worker node - delegating resources to cluster\")\n",
    "    stop_cluster = os.path.exists(stop_file_name)\n",
    "    \n",
    "    while not stop_cluster:\n",
    "        time.sleep(30)\n",
    "        print('Secondary worker - main thread - heartbeat')\n",
    "        stop_cluster = os.path.exists(stop_file_name)\n",
    "    print ('Shutting down worker')\n",
    "    ray.shutdown()\n",
    "    os.system(\"ray stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/jupyter/ray/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {NODE_DIR}/task.py\n",
    "from collections import Counter\n",
    "import socket\n",
    "import time\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from gym import spaces, logger\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# get random password pf length 8 with letters, digits\n",
    "characters = string.ascii_letters + string.digits\n",
    "ray_pwd = ''.join(random.choice(characters) for i in range(8))\n",
    "\n",
    "#Adding pip path\n",
    "os.environ[\"PATH\"] += os.pathsep + '/root/.local/bin'\n",
    "\n",
    "\n",
    "# Stop cluster file for cleanly shutting down workers\n",
    "model_dir = os.environ.get(\"AIP_MODEL_DIR\", 'trainer')\n",
    "\n",
    "print(f\"AIP_MODEL_DIR: {model_dir}\")\n",
    "\n",
    "'''\n",
    "start_file_name = 'cluster_started.txt'\n",
    "stop_file_name = 'stop_cluster.txt'\n",
    "\n",
    "bucket = model_dir.split(\"/\")[2]\n",
    "parent = (\"/\").join(model_dir.split(\"/\")[3:])\n",
    "stop_name = parent + stop_file_name\n",
    "start_name = parent + start_file_name\n",
    "\n",
    "stop_file_name = f\"/gcs/{bucket}/{stop_name}\"\n",
    "start_file_name = f\"/gcs/{bucket}/{start_name}\"\n",
    "\n",
    "'''\n",
    "\n",
    "# Stop cluster file for cleanly shutting down workers\n",
    "model_dir = os.getenv(\"AIP_MODEL_DIR\", 'trainer')\n",
    "print(f\"AIP_MODEL_DIR: {model_dir}\")\n",
    "\n",
    "start_file_name = 'cluster_started.txt'\n",
    "stop_file_name = 'stop_cluster.txt'\n",
    "\n",
    "bucket = model_dir.split(\"/\")[2]\n",
    "#bucket = \"vertexai-mlops-workshop\"\n",
    "#parent = (\"/\").join(model_dir.split(\"/\")[3:])\n",
    "parent = \"ray-rllib-custom-job\"\n",
    "stop_name = parent + stop_file_name\n",
    "start_name = parent + start_file_name\n",
    "\n",
    "print(f\"Stop file name: {stop_file_name}\")\n",
    "print(f\"Cluster Started file name: {start_file_name}\")\n",
    "\n",
    "#Cluster info\n",
    "cluster_spec = json.loads(os.environ.get(\"CLUSTER_SPEC\"))\n",
    "print(f\"ClusterSpec:\\n{cluster_spec}\")\n",
    "\n",
    "head_node = cluster_spec['cluster']['workerpool0'][0].split(\":\")[0]\n",
    "head_node_address = f'{head_node}:2222'\n",
    "\n",
    "#Grab the task variable to see which which node pool we are in.  \n",
    "#If workerpool0, we are the head node, otherwise, we are a worker node to delegate resources to the cluster\n",
    "\n",
    "head_node = (cluster_spec['task']['type'] == 'workerpool0')\n",
    "\n",
    "if head_node: \n",
    "    print(f\"Running as Head node: {head_node}\")\n",
    "    print(f\"Head node_address: {head_node_address}\")\n",
    "    os.system(f\"ray start --head --port=2222 --redis-password={ray_pwd} --dashboard-host=10.128.0.9\")\n",
    "    os.system(f\"mkdir -p /gcs/{bucket}/{parent}\")\n",
    "    os.system(f\"touch {start_file_name}\")\n",
    "\n",
    "else: # worker - connect and sleep\n",
    "    os.system(f\"ray start --address={head_node_address} --redis-password={ray_pwd} --dashboard-host=10.128.0.9\")\n",
    "\n",
    "ray.init(address=\"auto\",_redis_password=ray_pwd)\n",
    "\n",
    "\n",
    "#WORK\n",
    "class CartPole(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 50\n",
    "    }\n",
    "\n",
    "    def __init__(self, config: EnvContext):\n",
    "        #print(config)\n",
    "        #print(\"CUDA Here?: \", torch.cuda.is_available())\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "        self.env_count = config[\"env_count\"]\n",
    "        self.device=config[\"device\"]\n",
    "        \n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "        self.done = torch.full([self.env_count], True, dtype=torch.bool, device=self.device)\n",
    "        self.state = torch.zeros([self.env_count, 4], dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #breakpoint()\n",
    "        # All env must already have been reset.\n",
    "        self.done[:] = False\n",
    "        x, x_dot, theta, theta_dot = self.state[:, 0], self.state[:, 1], self.state[:, 2], self.state[:, 3]\n",
    "        #breakpoint()\n",
    "        force = self.force_mag * ((action * 2.) - 1.)\n",
    "        \n",
    "        costheta = torch.cos(theta)\n",
    "        sintheta = torch.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = ((self.gravity * sintheta - costheta * temp) \n",
    "                    / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state[:, 0], self.state[:, 1], self.state[:, 2], self.state[:, 3] = x, x_dot, theta, theta_dot\n",
    "\n",
    "        self.done = (\n",
    "            (x < -self.x_threshold)\n",
    "            | (x > self.x_threshold)\n",
    "            | (theta < -self.theta_threshold_radians)\n",
    "            | (theta > self.theta_threshold_radians)\n",
    "        )\n",
    "        reward = ~self.done\n",
    "        \n",
    "        #self.state = self.reset()\n",
    "        self.reset()\n",
    "        #print(\"In GPU: \", self.state.is_cuda)\n",
    "        to_return = self.state.to(\"cpu\").numpy()[0]\n",
    "        return to_return, reward.float().item(), self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        #breakpoint()\n",
    "        self.state = torch.where(self.done.unsqueeze(1), (torch.rand(self.env_count, 4, device=self.device) -0.5) / 10., self.state) \n",
    "        #self.state = (torch.rand((self.env_count, 4)) -0.5) / 10.\n",
    "        #print(\"Before converting to CPU, self.state=\", self.state)\n",
    "        #print(self.state)\n",
    "        to_return = self.state.to(\"cpu\").numpy()[0]\n",
    "        #print(\"After converting to CPU, to_return= \", to_return)\n",
    "        return to_return\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(.8, .6, .4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5, .5, .8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "if head_node:\n",
    "    \n",
    "    print('''This cluster consists of\n",
    "    {} nodes in total\n",
    "    {} CPU resources in total\n",
    "    {} GPU resources in total\n",
    "    '''.format(len(ray.nodes()), ray.cluster_resources()['CPU'],ray.cluster_resources()['GPU']))\n",
    "    \n",
    "    '''\n",
    "    tensorboard_dir = os.environ.get(\"AIP_TENSORBOARD_LOG_DIR\")\n",
    "    tb_bucket = tensorboard_dir.split(\"/\")[2]\n",
    "    tb_path = (\"/\").join(tensorboard_dir.split(\"/\")[3:])\n",
    "    '''\n",
    "    \n",
    "    #tensorboard_dir = os.environ.get(\"AIP_TENSORBOARD_LOG_DIR\")\n",
    "    tensorboard_dir = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\", 'tb_dir')\n",
    "    #tb_bucket = tensorboard_dir.split(\"/\")[2]\n",
    "    #tb_path = (\"/\").join(tensorboard_dir.split(\"/\")[3:])\n",
    "    tb_path = f\"{parent}/{tensorboard_dir}\"\n",
    "\n",
    "    ray_log_path = f\"/gcs/{bucket}/{tb_path}\"\n",
    "\n",
    "    print(f\"Ray Output Path: {ray_log_path}\")\n",
    "    \n",
    "    if not os.path.exists(ray_log_path):\n",
    "        os.makedirs(ray_log_path)\n",
    "\n",
    "    #RL WORK\n",
    "    register_env(\"CartPole\", lambda config: CartPole(config))\n",
    "    \n",
    "    #ENV config\n",
    "    config = dict(\n",
    "            {\n",
    "            \"num_workers\": 3,\n",
    "            \"num_gpus\": 0.5,\n",
    "            \"num_gpus_per_worker\": 1,\n",
    "            \"num_envs_per_worker\": 1,\n",
    "            \"num_multi_gpu_tower_stacks\": 1,\n",
    "            \"minibatch_buffer_size\": 1,\n",
    "            \"num_sgd_iter\" : 1,\n",
    "            \"vf_loss_coeff\": 0.5,\n",
    "            \"train_batch_size\": 1000,\n",
    "                #\"num_workers\": 3,\n",
    "                #\"num_gpus\": .5,\n",
    "                #\"num_gpus_per_worker\": 1,\n",
    "                #\"num_envs_per_worker\": 1,\n",
    "                #\"num_multi_gpu_tower_stacks\": 1,\n",
    "                #\"minibatch_buffer_size\": 1,\n",
    "                #\"num_sgd_iter\" : 1,\n",
    "                #\"vf_loss_coeff\": 0.5,\n",
    "                #\"train_batch_size\": 1000,\n",
    "            },\n",
    "            **{\n",
    "                \"env\": CartPole,\n",
    "                \"disable_env_checking\": True,\n",
    "                \"env_config\": {\n",
    "                    \"env_count\": 1,\n",
    "                    \"device\": \"cuda\",\n",
    "                },\n",
    "                \"model\": {\n",
    "                    \"fcnet_hiddens\": [256, 256],\n",
    "                    \"use_lstm\": True,\n",
    "                    \"lstm_cell_size\": 256,\n",
    "                    \"lstm_use_prev_action\": False,\n",
    "                    \"lstm_use_prev_reward\": False,\n",
    "                },\n",
    "                \"framework\": \"torch\",\n",
    "                # Run with tracing enabled for tfe/tf2?\n",
    "                \"eager_tracing\": False,\n",
    "            })\n",
    "    \n",
    "    stop = {\n",
    "        \"training_iteration\": 100\n",
    "    }\n",
    "\n",
    "    results = tune.run(\"IMPALA\", config=config, stop=stop, verbose=2, local_dir=ray_log_path, reuse_actors=True)\n",
    "\n",
    "\n",
    "    #Shutdown the cluster\n",
    "    print ('Shutting down cluster')\n",
    "    open(stop_file_name, 'w').close()\n",
    "    time.sleep(60) # Give workers a chance to detect the stop_cluster.txt file and cleanly shutdown\n",
    "    os.system(\"ray stop\")\n",
    "    os.remove(stop_file_name)\n",
    "    os.remove(start_file_name)\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"Worker node - delegating resources to cluster\")\n",
    "    stop_cluster = os.path.exists(stop_file_name)\n",
    "    \n",
    "    while not stop_cluster:\n",
    "        time.sleep(30)\n",
    "        print('Secondary worker - main thread - heartbeat')\n",
    "        stop_cluster = os.path.exists(stop_file_name)\n",
    "    print ('Shutting down worker')\n",
    "    ray.shutdown()\n",
    "    os.system(\"ray stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training using CustomTrainingJob with pre-built container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME=f\"ray-rllib-training-{TIMESTAMP}\"\n",
    "MACHINE_TYPE=\"a2-ultragpu-1g\"\n",
    "REPLICA_COUNT=1\n",
    "ACCELERATOR_TYPE=\"NVIDIA_TESLA_T4\"\n",
    "ACCELERATOR_COUNT=1\n",
    "WORKER_POOL_MACHINE_TYPE=\"n1-standard-4\"\n",
    "WORKER_POOL_REPLICA_COUNT=4\n",
    "WORKER_ACCELERATOR_TYPE=\"NVIDIA_TESLA_T4\"\n",
    "#Setup 1 accelerator per worker\n",
    "WORKER_ACCELERATOR_COUNT=1\n",
    "#EXECUTOR_IMAGE_URI=\"gcr.io/deeplearning-platform-release/base-cu113\"\n",
    "EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-10:latest\"\n",
    "WORKING_DIRECTORY=SETUP_DIR\n",
    "SCRIPT_PATH=\"/home/jupyter/ray/trainer/task.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://gcp-ml-sandbox-aip-20220420163359\n"
     ]
    }
   ],
   "source": [
    "print(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
      "gs://gcp-ml-sandbox-aip-20220420163359/aiplatform-2022-04-20-21:20:58.906-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://gcp-ml-sandbox-aip-20220420163359/aiplatform-custom-training-2022-04-20-21:20:58.987 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5649513394617712640?project=357746845324\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1365622578755928064?project=357746845324\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob run completed. Resource name: projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640\n",
      "WARNING:google.cloud.aiplatform.training_jobs:Training did not produce a Managed Model returning None. Training Pipeline projects/357746845324/locations/us-central1/trainingPipelines/5649513394617712640 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "display_name=JOB_NAME,\n",
    "script_path=SCRIPT_PATH,\n",
    "requirements=[\"ray[default]\",\"ray[rllib]\",\"gym\", \"tblib\"],\n",
    "container_uri=EXECUTOR_IMAGE_URI,\n",
    "staging_bucket=BUCKET_URI\n",
    ")\n",
    "\n",
    "job.run(\n",
    "replica_count=REPLICA_COUNT+WORKER_POOL_REPLICA_COUNT,\n",
    "accelerator_type=WORKER_ACCELERATOR_TYPE,\n",
    "machine_type=MACHINE_TYPE,\n",
    "accelerator_count=WORKER_ACCELERATOR_COUNT,\n",
    "network=\"projects/357746845324/global/networks/gcp-ml-sandbox-network\",\n",
    "enable_web_access=True,\n",
    "sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training using the local script and have Vertex autopackage the code\n",
    "Create job: https://cloud.google.com/vertex-ai/docs/training/create-custom-job#create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME=f\"ray-rllib-training-{TIMESTAMP}\"\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "ACCELERATOR_TYPE=\"NVIDIA_TESLA_T4\"\n",
    "ACCELERATOR_COUNT=1\n",
    "WORKER_POOL_MACHINE_TYPE=\"n1-standard-4\"\n",
    "WORKER_POOL_REPLICA_COUNT=1\n",
    "WORKER_ACCELERATOR_TYPE=\"NVIDIA_TESLA_T4\"\n",
    "#Setup 1 accelerator per worker\n",
    "WORKER_ACCELERATOR_COUNT=4\n",
    "EXECUTOR_IMAGE_URI=\"gcr.io/deeplearning-platform-release/base-cu113\"\n",
    "EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-10:latest\"\n",
    "WORKING_DIRECTORY=SETUP_DIR\n",
    "SCRIPT_PATH=\"trainer/task.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud ai custom-jobs create \\\n",
    "  --region=$REGION \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --worker-pool-spec=machine-type=$MACHINE_TYPE,replica-count=$REPLICA_COUNT,accelerator-type=$ACCELERATOR_TYPE,accelerator-count=$ACCELERATOR_COUNT,executor-image-uri=$EXECUTOR_IMAGE_URI,local-package-path=$WORKING_DIRECTORY,script=$SCRIPT_PATH \\\n",
    "  --worker-pool-spec=machine-type=$WORKER_POOL_MACHINE_TYPE,replica-count=$WORKER_POOL_REPLICA_COUNT,accelerator-type=$WORKER_ACCELERATOR_TYPE,accelerator-count=$WORKER_ACCELERATOR_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.8/subprocess.py:842: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "Sending build context to Docker daemon  20.53kB\n",
      "Step 1/10 : FROM us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-10:latest\n",
      " ---> 1004232908f3\n",
      "Step 2/10 : RUN mkdir -m 777 -p /usr/app /home\n",
      " ---> Running in 5b80db444fdd\n",
      "Removing intermediate container 5b80db444fdd\n",
      " ---> e7e95ad93f81\n",
      "Step 3/10 : WORKDIR /usr/app\n",
      " ---> Running in 3cde287716a3\n",
      "Removing intermediate container 3cde287716a3\n",
      " ---> 083c9ecad321\n",
      "Step 4/10 : ENV HOME=/home\n",
      " ---> Running in 5914fe28a2fc\n",
      "Removing intermediate container 5914fe28a2fc\n",
      " ---> f1cff28c0e47\n",
      "Step 5/10 : ENV PYTHONDONTWRITEBYTECODE=1\n",
      " ---> Running in 67ed5d166d22\n",
      "Removing intermediate container 67ed5d166d22\n",
      " ---> b962e39eae4e\n",
      "Step 6/10 : RUN rm -rf /var/sitecustomize\n",
      " ---> Running in c563d9bc3986\n",
      "Removing intermediate container c563d9bc3986\n",
      " ---> d16b6a9234c0\n",
      "Step 7/10 : COPY [\"./setup.py\", \"./setup.py\"]\n",
      " ---> baaccc6e8b5e\n",
      "Step 8/10 : RUN pip3 install --no-cache-dir .\n",
      " ---> Running in 4f29e4f807e9\n",
      "Processing /usr/app\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ray[rllib]\n",
      "  Downloading ray-1.12.0-cp37-cp37m-manylinux2014_x86_64.whl (53.2 MB)\n",
      "Collecting gym\n",
      "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting tblib\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from gym->trainer==0.1) (1.19.5)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.6-py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym->trainer==0.1) (2.0.0)\n",
      "Collecting importlib-metadata>=4.10.0\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (2.26.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (8.0.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (6.0)\n",
      "Collecting virtualenv\n",
      "  Downloading virtualenv-20.14.1-py2.py3-none-any.whl (8.8 MB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0\n",
      "  Downloading msgpack-1.0.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (1.2.0)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (1.43.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.6.0-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (3.19.1)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (21.2.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (4.3.1)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (1.2.0)\n",
      "Collecting gym\n",
      "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.7-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (143 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (1.3.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (1.7.3)\n",
      "Requirement already satisfied: matplotlib!=3.4.3 in /opt/conda/lib/python3.7/site-packages (from ray[rllib]->trainer==0.1) (3.5.1)\n",
      "Collecting lz4\n",
      "  Downloading lz4-4.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting tensorboardX>=1.9\n",
      "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.19.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.7/site-packages (from grpcio<=1.43.0,>=1.28.1->ray[rllib]->trainer==0.1) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.10.0->gym->trainer==0.1) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.10.0->gym->trainer==0.1) (4.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.4.3->ray[rllib]->trainer==0.1) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.4.3->ray[rllib]->trainer==0.1) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.4.3->ray[rllib]->trainer==0.1) (4.28.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.4.3->ray[rllib]->trainer==0.1) (8.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.4.3->ray[rllib]->trainer==0.1) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.4.3->ray[rllib]->trainer==0.1) (3.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.4.3->ray[rllib]->trainer==0.1) (1.3.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->ray[rllib]->trainer==0.1) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->ray[rllib]->trainer==0.1) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->ray[rllib]->trainer==0.1) (2021.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->ray[rllib]->trainer==0.1) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->ray[rllib]->trainer==0.1) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->ray[rllib]->trainer==0.1) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->ray[rllib]->trainer==0.1) (2.0.9)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2021.11.2-py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->ray[rllib]->trainer==0.1) (2.6.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->ray[rllib]->trainer==0.1) (1.2.0)\n",
      "Collecting imageio>=2.4.1\n",
      "  Downloading imageio-2.17.0-py3-none-any.whl (3.4 MB)\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /opt/conda/lib/python3.7/site-packages (from virtualenv->ray[rllib]->trainer==0.1) (2.3.0)\n",
      "Collecting distlib<1,>=0.3.1\n",
      "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
      "Building wheels for collected packages: trainer, gym\n",
      "  Building wheel for trainer (setup.py): started\n",
      "  Building wheel for trainer (setup.py): finished with status 'done'\n",
      "  Created wheel for trainer: filename=trainer-0.1-py3-none-any.whl size=1004 sha256=f678d861b69a32c308f4257b97daead59ddffa036ffc451fd3d89ed486cc2e3d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oqhqvbzl/wheels/12/03/c9/f722c02b66385adff7c7fa6d490848e7b252673926e52cfa41\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616822 sha256=c2c20563f1ba1e62d1bdeb010c53e6131dd1014407b291629139ebf766a9c279\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oqhqvbzl/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
      "Successfully built trainer gym\n",
      "Installing collected packages: importlib-metadata, filelock, distlib, virtualenv, tifffile, msgpack, imageio, tensorboardX, tabulate, scikit-image, ray, lz4, gym, dm-tree, tblib, trainer\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.9.0\n",
      "    Uninstalling importlib-metadata-4.9.0:\n",
      "      Successfully uninstalled importlib-metadata-4.9.0\n",
      "Successfully installed distlib-0.3.4 dm-tree-0.1.7 filelock-3.6.0 gym-0.21.0 imageio-2.17.0 importlib-metadata-4.11.3 lz4-4.0.0 msgpack-1.0.3 ray-1.12.0 scikit-image-0.19.2 tabulate-0.8.9 tblib-1.7.0 tensorboardX-2.5 tifffile-2021.11.2 trainer-0.1 virtualenv-20.14.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 4f29e4f807e9\n",
      " ---> 61306b380ebd\n",
      "Step 9/10 : COPY [\"trainer\", \"trainer\"]\n",
      " ---> df1fe3d47cab\n",
      "Step 10/10 : ENTRYPOINT [\"python3\", \"trainer/task.py\"]\n",
      " ---> Running in 26286cacb74f\n",
      "Removing intermediate container 26286cacb74f\n",
      " ---> fa6038d8904e\n",
      "Successfully built fa6038d8904e\n",
      "Successfully tagged gcr.io/anthos-workshop-cloudinsight/cloudai-autogenerated/ray-rllib-training-20220419015342:20220419.01.53.56.257804\n",
      "\n",
      "A custom container image is built locally.\n",
      "\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.8/subprocess.py:842: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "The push refers to repository [gcr.io/anthos-workshop-cloudinsight/cloudai-autogenerated/ray-rllib-training-20220419015342]\n",
      "80d086595c51: Preparing\n",
      "31c27f4103e8: Preparing\n",
      "771fc37b0824: Preparing\n",
      "bc1c40a7bfdc: Preparing\n",
      "89555bf0588c: Preparing\n",
      "d0f503eb5103: Preparing\n",
      "2090105d38de: Preparing\n",
      "e74f6867eca5: Preparing\n",
      "c0cceb8da3b6: Preparing\n",
      "96387584e6f9: Preparing\n",
      "25e7db83a591: Preparing\n",
      "a969e450a7a1: Preparing\n",
      "50fa655e62e9: Preparing\n",
      "0fb3e2c40a0c: Preparing\n",
      "84b94e326ba0: Preparing\n",
      "fd59bd0dee42: Preparing\n",
      "be085f68ddeb: Preparing\n",
      "01704e0f2d45: Preparing\n",
      "8570abcb0af5: Preparing\n",
      "8570abcb0af5: Preparing\n",
      "f76298b89e81: Preparing\n",
      "61969d6c4142: Preparing\n",
      "f2e39dc56ee8: Preparing\n",
      "12506010bbd1: Preparing\n",
      "19258949813a: Preparing\n",
      "8a3424f475cb: Preparing\n",
      "79de2e7105e3: Preparing\n",
      "80fbf1f9de72: Preparing\n",
      "95dd9c08140a: Preparing\n",
      "c666324d0075: Preparing\n",
      "96d4192bdeb8: Preparing\n",
      "335e5a8917a9: Preparing\n",
      "16bc9e3ff258: Preparing\n",
      "f792b1bf1944: Preparing\n",
      "eb91d6906cf7: Preparing\n",
      "4f6f4d888f36: Preparing\n",
      "11a0eb2e5b88: Preparing\n",
      "3acf9e54a0a1: Preparing\n",
      "f791f2c2530f: Preparing\n",
      "104b9a8c66b0: Preparing\n",
      "49cebc7e5651: Preparing\n",
      "7c603d846799: Preparing\n",
      "c3162fcd47c3: Preparing\n",
      "799788464538: Preparing\n",
      "37f2a0ff7c81: Preparing\n",
      "53f2a227f3e7: Preparing\n",
      "db9361c17aeb: Preparing\n",
      "d40a0ad8471d: Preparing\n",
      "2778847217d7: Preparing\n",
      "4942a1abcbfa: Preparing\n",
      "84b94e326ba0: Waiting\n",
      "fd59bd0dee42: Waiting\n",
      "be085f68ddeb: Waiting\n",
      "01704e0f2d45: Waiting\n",
      "8570abcb0af5: Waiting\n",
      "f76298b89e81: Waiting\n",
      "61969d6c4142: Waiting\n",
      "f2e39dc56ee8: Waiting\n",
      "12506010bbd1: Waiting\n",
      "19258949813a: Waiting\n",
      "8a3424f475cb: Waiting\n",
      "d0f503eb5103: Waiting\n",
      "2090105d38de: Waiting\n",
      "79de2e7105e3: Waiting\n",
      "e74f6867eca5: Waiting\n",
      "80fbf1f9de72: Waiting\n",
      "49cebc7e5651: Waiting\n",
      "7c603d846799: Waiting\n",
      "c3162fcd47c3: Waiting\n",
      "799788464538: Waiting\n",
      "37f2a0ff7c81: Waiting\n",
      "53f2a227f3e7: Waiting\n",
      "db9361c17aeb: Waiting\n",
      "d40a0ad8471d: Waiting\n",
      "2778847217d7: Waiting\n",
      "4942a1abcbfa: Waiting\n",
      "c0cceb8da3b6: Waiting\n",
      "96387584e6f9: Waiting\n",
      "25e7db83a591: Waiting\n",
      "95dd9c08140a: Waiting\n",
      "c666324d0075: Waiting\n",
      "96d4192bdeb8: Waiting\n",
      "eb91d6906cf7: Waiting\n",
      "4f6f4d888f36: Waiting\n",
      "11a0eb2e5b88: Waiting\n",
      "3acf9e54a0a1: Waiting\n",
      "335e5a8917a9: Waiting\n",
      "16bc9e3ff258: Waiting\n",
      "f791f2c2530f: Waiting\n",
      "f792b1bf1944: Waiting\n",
      "104b9a8c66b0: Waiting\n",
      "a969e450a7a1: Waiting\n",
      "50fa655e62e9: Waiting\n",
      "0fb3e2c40a0c: Waiting\n",
      "771fc37b0824: Pushed\n",
      "80d086595c51: Pushed\n",
      "bc1c40a7bfdc: Pushed\n",
      "d0f503eb5103: Layer already exists\n",
      "2090105d38de: Layer already exists\n",
      "89555bf0588c: Pushed\n",
      "c0cceb8da3b6: Layer already exists\n",
      "96387584e6f9: Layer already exists\n",
      "e74f6867eca5: Layer already exists\n",
      "25e7db83a591: Layer already exists\n",
      "a969e450a7a1: Layer already exists\n",
      "50fa655e62e9: Layer already exists\n",
      "0fb3e2c40a0c: Layer already exists\n",
      "84b94e326ba0: Layer already exists\n",
      "fd59bd0dee42: Layer already exists\n",
      "be085f68ddeb: Layer already exists\n",
      "01704e0f2d45: Layer already exists\n",
      "8570abcb0af5: Layer already exists\n",
      "61969d6c4142: Layer already exists\n",
      "f2e39dc56ee8: Layer already exists\n",
      "f76298b89e81: Layer already exists\n",
      "12506010bbd1: Layer already exists\n",
      "19258949813a: Layer already exists\n",
      "8a3424f475cb: Layer already exists\n",
      "79de2e7105e3: Layer already exists\n",
      "80fbf1f9de72: Layer already exists\n",
      "95dd9c08140a: Layer already exists\n",
      "c666324d0075: Layer already exists\n",
      "96d4192bdeb8: Layer already exists\n",
      "335e5a8917a9: Layer already exists\n",
      "16bc9e3ff258: Layer already exists\n",
      "eb91d6906cf7: Layer already exists\n",
      "f792b1bf1944: Layer already exists\n",
      "11a0eb2e5b88: Layer already exists\n",
      "f791f2c2530f: Layer already exists\n",
      "3acf9e54a0a1: Layer already exists\n",
      "4f6f4d888f36: Layer already exists\n",
      "49cebc7e5651: Layer already exists\n",
      "104b9a8c66b0: Layer already exists\n",
      "7c603d846799: Layer already exists\n",
      "c3162fcd47c3: Layer already exists\n",
      "37f2a0ff7c81: Layer already exists\n",
      "799788464538: Layer already exists\n",
      "53f2a227f3e7: Layer already exists\n",
      "db9361c17aeb: Layer already exists\n",
      "d40a0ad8471d: Layer already exists\n",
      "2778847217d7: Layer already exists\n",
      "4942a1abcbfa: Layer already exists\n",
      "31c27f4103e8: Pushed\n",
      "20220419.01.53.56.257804: digest: sha256:b67e901c23ccb08bb4796cd68dbda8702159fc0f8983b25194f24ee0eaabda15 size: 10773\n",
      "\n",
      "Custom container image [gcr.io/anthos-workshop-cloudinsight/cloudai-autogenerated/ray-rllib-training-20220419015342:20220419.01.53.56.257804] is created for your custom job.\n",
      "\n",
      "CustomJob [projects/780788467724/locations/us-central1/customJobs/6567315332740939776] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/780788467724/locations/us-central1/customJobs/6567315332740939776\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/780788467724/locations/us-central1/customJobs/6567315332740939776\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai custom-jobs create \\\n",
    "  --region=$REGION \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --worker-pool-spec=machine-type=$MACHINE_TYPE,replica-count=$REPLICA_COUNT,executor-image-uri=$EXECUTOR_IMAGE_URI,local-package-path=$WORKING_DIRECTORY,script=$SCRIPT_PATH \\\n",
    "  --worker-pool-spec=machine-type=$WORKER_POOL_MACHINE_TYPE,replica-count=$WORKER_POOL_REPLICA_COUNT,accelerator-type=$WORKER_ACCELERATOR_TYPE,accelerator-count=$WORKER_ACCELERATOR_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy command from above to view job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "createTime: '2022-04-19T01:54:53.431584Z'\n",
      "displayName: ray-rllib-training-20220419015342\n",
      "endTime: '2022-04-19T02:00:30Z'\n",
      "error:\n",
      "  code: 3\n",
      "  message: 'The replica workerpool0-0 exited with a non-zero status of 1. Termination\n",
      "    reason: Error. To find out more about why your job exited please check the logs:\n",
      "    https://console.cloud.google.com/logs/viewer?project=780788467724&resource=ml_job%2Fjob_id%2F6567315332740939776&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%226567315332740939776%22'\n",
      "jobSpec:\n",
      "  workerPoolSpecs:\n",
      "  - containerSpec:\n",
      "      imageUri: gcr.io/anthos-workshop-cloudinsight/cloudai-autogenerated/ray-rllib-training-20220419015342:20220419.01.53.56.257804\n",
      "    diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      machineType: n1-standard-4\n",
      "    replicaCount: '1'\n",
      "  - containerSpec:\n",
      "      imageUri: gcr.io/anthos-workshop-cloudinsight/cloudai-autogenerated/ray-rllib-training-20220419015342:20220419.01.53.56.257804\n",
      "    diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      acceleratorCount: 4\n",
      "      acceleratorType: NVIDIA_TESLA_T4\n",
      "      machineType: n1-standard-4\n",
      "    replicaCount: '1'\n",
      "name: projects/780788467724/locations/us-central1/customJobs/6567315332740939776\n",
      "startTime: '2022-04-19T02:00:00Z'\n",
      "state: JOB_STATE_FAILED\n",
      "updateTime: '2022-04-19T02:00:40.823851Z'\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai custom-jobs describe projects/780788467724/locations/us-central1/customJobs/6567315332740939776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
